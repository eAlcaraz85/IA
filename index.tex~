% Created 2024-11-26 mar 09:55
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage[spanish]{inputenc}
\author{likcos}
\date{\today}
\title{Apuntes IA}
\hypersetup{
 pdfauthor={likcos},
 pdftitle={Apuntes IA},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 28.2 (Org mode 9.5.5)}, 
 pdflang={Spanish}}
\begin{document}

\maketitle
\tableofcontents




\section*{Introducción a la Inteligencia Artificial.}
\label{sec:org19abe54}
La inteligencia artificial (IA) es un área multidisciplinaria que, a
 través de ciencias como las ciencias de la computación, la lógica y la
 filosofía, estudia la creación y diseño de entidades capaces de
 resolver cuestiones por sí mismas utilizando como paradigma la
 inteligencia humana.


General y amplio como eso, reúne a amplios campos, los cuales tienen
en común la creación de máquinas capaces de pensar. En ciencias de la
computación se denomina inteligencia artificial a la capacidad de
razonar de un agente no vivo. John McCarthy acuñó la expresión
inteligencia artificial en 1956, y la definió así: Es la ciencia e
ingenio de hacer máquinas inteligentes, especialmente programas de
cómputo inteligentes.

\begin{itemize}
\item Búsqueda del estado requerido en el conjunto de los estados
producidos por las acciones posibles.
\item Algoritmos genéticos (análogo al proceso de evolución de las cadenas
de ADN).
\item Redes neuronales artificiales (análogo al funcionamiento físico del
cerebro de animales y humanos).
\item Razonamiento mediante una lógica formal análogo al pensamiento
abstracto humano.
\end{itemize}


También existen distintos tipos de percepciones y acciones, que pueden
ser obtenidas y producidas, respectivamente, por sensores físicos y
sensores mecánicos en máquinas, pulsos eléctricos u ópticos en
computadoras, tanto como por entradas y salidas de bits de un software
y su entorno software.
Varios ejemplos se encuentran en el área de control de sistemas,
planificación automática, la habilidad de responder a diagnósticos y a
consultas de los consumidores, reconocimiento de escritura,
reconocimiento del habla y reconocimiento de patrones. Los sistemas de
IA actualmente son parte de la rutina en campos como economía,
medicina, ingeniería y la milicia, y se ha usado en gran variedad de
aplicaciones de software, juegos de estrategia, como ajedrez de
computador, y otros videojuegos.

\subsection*{Categorías de la Inteligencia Artificial}
\label{sec:org8e6eb86}

\begin{itemize}
\item \textbf{Sistemas que piensan como humanos}. Estos sistemas tratan de emular
el pensamiento humano; por ejemplo las redes neuronales
artificiales. La automatización de actividades que vinculamos con
procesos de pensamiento humano, actividades como la Toma de
decisiones, Resolución de problemas y aprendizaje. \cite{Russell}
\item \textbf{Sistemas que actúan como humanos}: Estos sistemas tratan de actuar
como humanos; es decir, imitan el comportamiento humano; por
ejemplo la robótica. El estudio de cómo lograr que los
computadores realicen tareas que, por el momento, los humanos hacen mejor. \cite{Russell}
\item \textbf{Sistemas que piensan racionalmente}.- Es decir, con lógica (idealmente), tratan de
imitar o emular el pensamiento lógico racional del ser humano; por ejemplo los sistemas
expertos. El estudio de los cálculos que hacen posible percibir, razonar y actuar. \cite{Russell}
\item \textbf{Sistemas que actúan racionalmente (idealmente)}.– Tratan de emular de forma
racional el comportamiento humano; por ejemplo los agentes inteligentes.Está relacionado con
conductas inteligentes en artefactos. \cite{Russell}
\end{itemize}

\subsection*{Inteligencia artificial convencional}
\label{sec:org54afa9f}

Se conoce también como IA simbólico-deductiva. Está basada en el análisis formal y
estadístico del comportamiento humano ante diferentes problemas:

\begin{itemize}
\item Razonamiento basado en casos: Ayuda a tomar decisiones mientras se
resuelven ciertos problemas concretos y, aparte de que son muy
importantes, requieren de un buen funcionamiento.
\item Sistemas expertos: Infieren una solución a través del conocimiento
previo del contexto en que se aplica y ocupa de ciertas reglas o
relaciones.
\item Redes bayesianas: Propone soluciones mediante inferencia probabilística.
\item Inteligencia artificial basada en comportamientos: Esta
inteligencia contiene autonomía y puede auto-regularse y
controlarse para mejorar.
\item Smart process management: Facilita la toma de decisiones
complejas, proponiendo una solución a un determinado problema al
igual que lo haría un especialista en la dicha actividad.
\end{itemize}


\subsection*{Historia de la Inteligencia Artificial.}
\label{sec:org539bdc0}
\subsection*{Las habilidades cognoscitivas según la psicología. Teorías de la inteligencia (conductismo, Gardner, etc.).}
\label{sec:org4a4b569}

Las habilidades cognoscitivas son capacidades mentales que nos
permiten procesar toda la información que nos llega del
entorno. Incluyen procesos como la percepción, la memoria, el
aprendizaje, la solución de problemas, el razonamiento y el
pensamiento crítico. La psicología ha estudiado extensamente estas
habilidades, y diferentes teorías han surgido para explicar cómo se
desarrollan y funcionan, especialmente en relación con la
inteligencia.

\subsubsection*{Teorías de la Inteligencia}
\label{sec:orgb1b8d6e}

\begin{enumerate}
\item \textbf{Conductismo} El conductismo, representado por figuras como
John B. Watson y B.F. Skinner, se centra en el estudio de
comportamientos observables y medibles, dejando de lado los procesos
mentales internos. Desde esta perspectiva, la inteligencia se ve como
una serie de respuestas aprendidas ante estímulos específicos. Los
conductistas creen que cualquier diferencia en la inteligencia entre
individuos se debe a diferencias en sus experiencias de
aprendizaje. Aunque esta teoría ha sido crítica por su falta de
atención a los procesos cognitivos internos, ha sido influyente en el
desarrollo de técnicas de aprendizaje y modificación del
comportamiento.

\item \textbf{Teoría de las Inteligencias Múltiples (Howard Gardner)}
Contrastando con el enfoque unitario de la inteligencia, Howard
Gardner propuso en 1983 la Teoría de las Inteligencias
Múltiples. Gardner argumentó que la inteligencia no es un dominio
único y general, sino un conjunto de capacidades cognitivas distintas
e independientes. Originalmente identificó siete inteligencias
(lingüístico-verbal, lógico-matemática, espacial, musical,
corporal-cinestésica, interpersonal e intrapersonal), a las cuales
más tarde añadió la inteligencia naturalista y posiblemente
otras. Esta teoría ha tenido un impacto significativo en la
educación, promoviendo un enfoque más personalizado en la enseñanza.

\item \textbf{Teoría Triárquica de la Inteligencia (Robert Sternberg)}
Robert Sternberg propuso la Teoría Triárquica de la Inteligencia, que
divide la inteligencia en tres aspectos: analítico, creativo y
práctico. La inteligencia analítica se refiere a la capacidad de
analizar, evaluar, juzgar, comparar y contrastar. La inteligencia
creativa implica la capacidad de crear, diseñar, inventar, originar y
imaginar. La inteligencia práctica se relaciona con la capacidad de
usar, aplicar, implementar y poner en práctica. Sternberg sugiere que
una inteligencia equilibrada implica la capacidad de adaptarse a,
moldear y seleccionar entornos para satisfacer tanto las necesidades
personales como las de la sociedad.

\item \textbf{Teoría del Procesamiento de la Información}
Esta teoría se enfoca en cómo las personas procesan la información
que reciben. Implica la atención, percepción, memoria y pensamiento,
y cómo estas operaciones mentales influyen en nuestra capacidad para
resolver problemas y tomar decisiones. Desde esta perspectiva, la
inteligencia se ve como un conjunto de procesos mentales que permiten
a la persona comprender y manejar el mundo que la rodea.

\item \textbf{Inteligencia Emocional (Daniel Goleman)}
Daniel Goleman popularizó el concepto de inteligencia emocional en
los años 90, definiéndola como la capacidad para reconocer, entender
y manejar nuestras emociones y las de los demás. La inteligencia
emocional incluye habilidades como la autoconciencia, la
autoregulación, la motivación, la empatía y las habilidades
sociales. Esta teoría amplió el concepto de inteligencia más allá de
las capacidades cognitivas tradicionales, incluyendo aspectos
emocionales y sociales.
\end{enumerate}

\textbf{Conclusión}: Las teorías de la inteligencia en la psicología
reflejan la complejidad y diversidad de la mente humana. Desde el
conductismo, que enfatiza el aprendizaje observable, hasta las
teorías de inteligencias múltiples y emocional, que reconocen una
amplia gama de capacidades cognitivas y emocionales, estas teorías
nos ofrecen diferentes puntos de vista a través de los cuales podemos entender
la inteligencia humana. Cada teoría aporta su visión única,
sugiriendo que la inteligencia es un fenómeno multifacético que no
puede ser completamente comprendido a través de un solo enfoque.

\subsection*{El proceso de razonamiento según la lógica (Axiomas, Teoremas, demostración).}
\label{sec:orgfeb75c9}
El proceso de razonamiento según la lógica involucra varios conceptos
fundamentales como axiomas, teoremas y demostraciones. Estos elementos
forman la base de la lógica y el razonamiento matemático, permitiendo
construir argumentos sólidos y verificar la veracidad de diversas
proposiciones.

\begin{itemize}
\item \textbf{Axiomas} Los axiomas son declaraciones o proposiciones que se aceptan
como verdaderas sin necesidad de demostración. En la lógica y las
matemáticas, los axiomas sirven como fundamentos sobre los cuales se
construye todo el sistema teórico. No son arbitrarios; se eligen
cuidadosamente para evitar contradicciones y para ser lo
suficientemente potentes como para derivar teoremas relevantes. Un
ejemplo clásico de axioma es el postulado de paralelas de Euclides,
que afirma que por un punto exterior a una línea, se puede trazar
una y solo una paralela a dicha línea.

\item \textbf{Teoremas} Los teoremas son proposiciones que han sido demostradas
como verdaderas dentro de un sistema lógico, utilizando axiomas y
teoremas previamente establecidos. Los teoremas requieren una
demostración rigurosa que muestre su veracidad. Un ejemplo famoso es
el teorema de Pitágoras en la geometría euclidiana, el cual
establece que en un triángulo rectángulo, el cuadrado de la
hipotenusa (el lado opuesto al ángulo recto) es igual a la suma de
los cuadrados de los otros dos lados.

\item \textbf{Demostración} La demostración es el proceso mediante el cual se
establece la verdad de un teorema. Utiliza una serie de pasos
lógicos y deductivos, basados en axiomas y en teoremas ya
demostrados, para llegar a la conclusión de que el teorema en
cuestión es verdadero. Las demostraciones pueden adoptar diversas
formas, como la demostración directa, donde se parte de los axiomas
y se llega al teorema; la demostración por contradicción, donde se
asume que el teorema es falso y se llega a un absurdo; y la
demostración por inducción, útil especialmente para los teoremas que
involucran números enteros.
\end{itemize}

Este proceso es fundamental en el razonamiento lógico y matemático, ya
que proporciona una base sólida para entender y verificar la verdad de
las proposiciones dentro de un marco teórico específico. A través de
los axiomas, teoremas y demostraciones, la lógica facilita la
construcción de conocimiento estructurado y coherente, permitiendo el
avance y la aplicación de las matemáticas y otras disciplinas que
dependen de la lógica formal.

\subsection*{El modelo de adquisición del conocimiento según la filosofía.}
\label{sec:org4e82eb9}
El modelo de adquisición del conocimiento según la filosofía aborda
cómo los seres humanos entienden, aprenden y conocen el mundo que les
rodea. Esta cuestión ha sido central en la filosofía desde sus
inicios, involucrando a filósofos de todas las épocas, desde los
antiguos hasta los contemporáneos. La filosofía del conocimiento, o
epistemología, estudia la naturaleza, el origen y los límites del
conocimiento. A lo largo de la historia, se han propuesto varios
modelos para explicar cómo adquirimos conocimiento, incluyendo el
empirismo, el racionalismo, el constructivismo, y la fenomenología,
entre otros.

\begin{itemize}
\item \textbf{Empirismo}: El empirismo sostiene que el conocimiento proviene de la
experiencia sensorial. Según esta visión, todos los conceptos son
derivados de la experiencia y la mente al nacer es una tabula rasa,
una hoja en blanco sobre la cual la experiencia escribe. John Locke,
George Berkeley y David Hume son algunos de los filósofos más
destacados asociados con el empirismo. Por ejemplo, Locke argumentó
que el conocimiento se construye a partir de ideas simples que se
obtienen a través de la experiencia y que estas ideas simples se
combinan para formar ideas complejas.

\item \textbf{Racionalismo}: En contraposición al empirismo, el racionalismo
argumenta que el conocimiento se adquiere principalmente a través de
la razón y la intuición, más que a través de los sentidos. Los
racionalistas creen en la existencia de ideas innatas, es decir,
conocimientos que nacen con el individuo. René Descartes, Baruch
Spinoza y Gottfried Wilhelm Leibniz son figuras clave del
racionalismo. Descartes, por ejemplo, propuso el método de la duda
sistemática y llegó a la conclusión de que la única certeza es
"Cogito, ergo sum" ("Pienso, luego existo"), subrayando la primacía
de la mente y la razón en la adquisición del conocimiento.

\item \textbf{Constructivismo:} El constructivismo sostiene que el conocimiento se
construye activamente por el cognoscente, no es simplemente una
copia de la realidad. Esta teoría sugiere que los individuos
construyen su conocimiento a través de la interacción con el entorno
y mediante la reinterpretación de sus experiencias a la luz de sus
propias creencias y antecedentes. Jean Piaget es uno de los teóricos
más influyentes en esta área, argumentando que el desarrollo
cognitivo del niño se produce a través de una serie de etapas y que
el aprendizaje es un proceso de reorganización de estructuras
mentales.

\item \textbf{Fenomenología:} La fenomenología es un enfoque filosófico que
enfatiza la experiencia subjetiva como la fuente principal del
conocimiento. Fundada por Edmund Husserl, la fenomenología busca
describir los fenómenos tal como se presentan a la conciencia, sin
recurrir a teorías o interpretaciones previas. Este enfoque ha
influenciado a muchos filósofos y teóricos, incluyendo a Martin
Heidegger y Maurice Merleau-Ponty, y se centra en la comprensión de
la experiencia vivida desde el punto de vista de la primera persona.
\end{itemize}

\textbf{Conclusión} El modelo de adquisición del conocimiento según la
filosofía no se limita a una única teoría o enfoque. En cambio,
refleja una rica diversidad de perspectivas sobre cómo los humanos
llegan a entender el mundo. Cada enfoque ofrece una visión diferente
sobre la naturaleza del conocimiento, cómo se adquiere, y los límites
de nuestra comprensión. La epistemología sigue siendo un campo de
estudio vibrante y en evolución, que continúa desafiando nuestras
concepciones sobre la mente, la realidad y la forma en que
interactuamos con el mundo que nos rodea.

\subsection*{El modelo cognoscitivo.}
\label{sec:org29bc1a0}
El modelo cognoscitivo es un enfoque teórico en la psicología que pone
énfasis en la comprensión de los procesos mentales internos que
subyacen a la percepción, el pensamiento, el aprendizaje, la memoria y
la resolución de problemas. Contrario a los modelos de conducta que se
enfocan únicamente en las respuestas observables a los estímulos, el
modelo cognoscitivo busca entender cómo las personas interpretan,
procesan y almacenan la información recibida del entorno. Este modelo
ha sido fundamental en el desarrollo de la psicología cognitiva, una
rama de la psicología que estudia los procesos mentales internos.


\subsubsection*{Fundamentos del Modelo Cognoscitivo}
\label{sec:orgc2270c2}

El modelo cognoscitivo se basa en la premisa de que la mente funciona
de manera similar a un ordenador: recibe datos (inputs), los procesa
y luego produce respuestas (outputs). Este enfoque enfatiza la
importancia de los procesos mentales internos y cómo estos influyen
en la conducta. Los aspectos clave del modelo cognoscitivo incluyen:

\begin{itemize}
\item \textbf{Percepción:} Cómo interpretamos y damos sentido a la información
sensorial del mundo que nos rodea.
\item \textbf{Atención:} Cómo filtramos y seleccionamos información del entorno
para procesarla más a fondo.
\item \textbf{Memoria:} Cómo almacenamos y recuperamos información. La memoria se
considera en diferentes formatos, como la memoria a corto plazo (o
memoria de trabajo) y la memoria a largo plazo.
\item \textbf{Pensamiento:} Cómo solucionamos problemas, tomamos decisiones y
ejecutamos el razonamiento lógico.
\item \textbf{Lenguaje:} Cómo utilizamos el lenguaje para pensar, comunicarnos y entender el mundo.
\end{itemize}


\subsubsection*{Aplicaciones del Modelo Cognoscitivo}
\label{sec:org0f770f0}

El modelo cognoscitivo ha tenido aplicaciones extensas en varios campos, incluyendo:

\begin{itemize}
\item \textbf{Educación:} Desarrollo de estrategias de enseñanza basadas en cómo
los estudiantes procesan y recuerdan la información.
\item \textbf{Terapia Cognitiva:} En psicología clínica, se utiliza para tratar
trastornos como la depresión y la ansiedad, ayudando a los
pacientes a reconocer y cambiar patrones de pensamiento
distorsionados.
\item \textbf{Diseño de Interfaces de Usuario:} En la tecnología de la
información, se aplica para crear sistemas que se alineen mejor con
los procesos cognitivos humanos, haciendo que las interfaces sean
más intuitivas.
\end{itemize}

\subsubsection*{Críticas y Limitaciones}
\label{sec:org40d77ce}

A pesar de su influencia y aplicabilidad, el modelo cognoscitivo también ha enfrentado críticas, principalmente por:

\begin{itemize}
\item \textbf{Reduccionismo:} Algunos críticos argumentan que reduce los procesos
mentales complejos a simples mecanismos computacionales.
\item \textbf{Descuido de lo Emocional y lo Social:} Inicialmente, el modelo
cognoscitivo fue criticado por ignorar cómo las emociones y el
contexto social afectan el procesamiento cognitivo.
\end{itemize}

\subsubsection*{Evolución y Expansión}
\label{sec:orga417ff7}
En respuesta a estas críticas, el modelo cognoscitivo ha evolucionado
para incorporar aspectos emocionales y sociales en el estudio de los
procesos mentales. Esto ha llevado al desarrollo de subcampos como la
psicología cognitiva social y la neurociencia cognitiva, que exploran
la interacción entre cognición, emoción y contextos sociales.

\subsubsection*{Conclusión}
\label{sec:org7479e82}
El modelo cognoscitivo ha sido fundamental en el avance de nuestra
comprensión de los procesos mentales y ha revolucionado la forma en
que los psicólogos abordan el estudio de la mente y la conducta. A
través de su aplicación en educación, terapia y tecnología, ha
demostrado ser una herramienta invaluable para mejorar diversos
aspectos de la vida humana. Con el tiempo, continúa adaptándose y
expandiéndose para incluir una comprensión más holística de la
cognición humana.


\subsection*{El modelo del agente inteligente, Sistemas Multi Agentes, Sistemas Ubicuos.}
\label{sec:org3a616d0}
El modelo del agente inteligente, los Sistemas Multi-Agente (SMA) y
los Sistemas Ubicuos son conceptos fundamentales en el ámbito de la
inteligencia artificial (IA) y las ciencias de la computación, que
tienen aplicaciones en una amplia gama de dominios, desde la
automatización del hogar hasta la manufactura avanzada y los entornos
de trabajo colaborativos. A continuación, se desarrolla cada uno de
estos conceptos para proporcionar una comprensión clara de su
significado, funcionamiento y aplicaciones.

\subsubsection*{El Modelo del Agente Inteligente}
\label{sec:orgb13c6ad}

Un agente inteligente es una entidad autónoma capaz de percibir su
entorno a través de sensores y actuar en ese entorno utilizando
actuadores para lograr ciertos objetivos o maximizar una medida de
rendimiento. Los agentes inteligentes pueden ser simples, como un
termostato programado para mantener una temperatura específica, o
complejos, como un robot autónomo explorando Marte. La inteligencia
de un agente se refleja en su capacidad para tomar decisiones
adecuadas en función de sus percepciones y los objetivos
establecidos.


\subsubsection*{Sistemas Multi-Agente (SMA)}
\label{sec:orgf6b932e}

Los Sistemas Multi-Agente son sistemas compuestos por varios agentes
inteligentes que interactúan entre sí dentro de un entorno. Estos
agentes pueden cooperar, competir o negociar para lograr sus
objetivos individuales o colectivos. Los SMA son especialmente útiles
para resolver problemas que son demasiado complejos o grandes para
ser abordados por un único agente. Los ejemplos de aplicaciones
incluyen la simulación de sistemas sociales, la optimización de
procesos de manufactura, la gestión de redes de transporte y los
juegos serios para la educación y la formación.


\subsubsection*{Sistemas Ubicuos}
\label{sec:org434c768}

Los Sistemas Ubicuos, también conocidos como computación ubicua, se
refieren a la integración de la computación en el entorno cotidiano
de manera que los dispositivos computacionales estén disponibles en
todo momento y lugar, pero de manera invisible para el usuario. Estos
sistemas se caracterizan por su capacidad para proporcionar servicios
y soporte de manera proactiva y context-aware, es decir, adaptando su
comportamiento según el contexto del usuario. Los ejemplos incluyen
casas inteligentes que ajustan automáticamente la iluminación y la
temperatura, ciudades inteligentes con gestión avanzada del tráfico y
sistemas de alerta temprana, y dispositivos personales de salud que
monitorizan constantemente el bienestar del usuario.


\subsubsection*{Integración y Aplicaciones}
\label{sec:orgfdd0c6e}
La integración de estos conceptos permite el desarrollo de soluciones
innovadoras a problemas complejos. Por ejemplo, un SMA puede ser
desplegado en un entorno ubicuo para gestionar de manera eficiente y
adaptativa los recursos energéticos de una ciudad inteligente. Los
agentes inteligentes, actuando dentro de este sistema, pueden
monitorear el consumo de energía en tiempo real, predecir la demanda
futura y ajustar de manera proactiva la distribución de energía para
maximizar la eficiencia y minimizar los costos.

\subsubsection*{Conclusión}
\label{sec:org5538be5}
El modelo del agente inteligente, los Sistemas Multi-Agente y los
Sistemas Ubicuos representan áreas clave en la investigación y
aplicación de la inteligencia artificial y la computación. Al combinar
la autonomía y la capacidad de toma de decisiones de los agentes
inteligentes con la cooperación y la interacción de los SMA, y al
integrar estos sistemas en el entorno cotidiano a través de la
computación ubicua, es posible desarrollar soluciones avanzadas y
adaptativas para una amplia gama de desafíos en la sociedad moderna.

\subsection*{El papel de la heurística.}
\label{sec:org4f695f1}

\textbf{Búsqueda Heurística:} En la búsqueda heurística, las técnicas
heurísticas se utilizan para acelerar el proceso de búsqueda de
soluciones, especialmente en problemas de búsqueda en espacios de
estados grandes, como los encontrados en la planificación, juegos de
estrategia, y la resolución de puzzles. Algoritmos como A* y sus
variantes utilizan funciones heurísticas para estimar el costo del
camino más corto desde un nodo dado hasta el objetivo, priorizando la
exploración de caminos que parecen más prometedores.

\begin{itemize}
\item \textbf{Optimización} Las heurísticas también son cruciales en problemas de
optimización, donde se busca la mejor solución de entre un conjunto
de soluciones posibles. Técnicas como la búsqueda tabú, algoritmos
genéticos y el recocido simulado son ejemplos de métodos heurísticos
que exploran el espacio de soluciones de manera estratégica para
encontrar soluciones óptimas o cercanas al óptimo en un tiempo
razonable.

\item \textbf{Toma de Decisiones}: En la toma de decisiones, las heurísticas ayudan
a simplificar los procesos de evaluación y elección al reducir la
complejidad de las decisiones y la cantidad de información a
considerar. Esto es especialmente útil en la IA para juegos, donde
los agentes deben tomar decisiones rápidas y efectivas en entornos
competitivos con información incompleta.

\item \textbf{Diseño de Algoritmos}: Las heurísticas son fundamentales en el diseño
de algoritmos para el procesamiento de lenguaje natural, visión por
computadora, y sistemas de recomendación, donde guían el análisis y
la interpretación de datos complejos o no estructurados para
realizar tareas como la clasificación, el reconocimiento de patrones
y la predicción.
\end{itemize}

\subsubsection*{Ventajas y Limitaciones}
\label{sec:orgdcf2202}
Las principales ventajas de utilizar heurísticas en la IA incluyen la
capacidad de encontrar soluciones de manera más rápida y eficiente que
los métodos exhaustivos, especialmente en problemas complejos o de
gran escala. Sin embargo, el uso de heurísticas también tiene sus
limitaciones, ya que la solución encontrada no siempre es la óptima, y
la calidad de la solución puede depender significativamente de la
elección de la función heurística.

\begin{itemize}
\item \textbf{Conclusión}: La heurística es una herramienta invaluable en la
inteligencia artificial, permitiendo el desarrollo de sistemas y
algoritmos que pueden operar efectivamente en entornos complejos y
bajo restricciones de tiempo o recursos. Aunque la elección y el
diseño de funciones heurísticas adecuadas pueden ser desafiantes, su
aplicación sigue siendo esencial para el avance y la eficacia de la
IA en una amplia gama de aplicaciones prácticas.
\end{itemize}



\subsubsection*{Algoritmos de exploración de alternativas.}
\label{sec:org7c96316}



\subsubsection*{Algoritmo A*}
\label{sec:org5e34d06}

El \textbf{algoritmo A*} es uno de los algoritmos de búsqueda más populares
y eficientes para encontrar el camino más corto entre dos puntos,
especialmente en grafos o espacios de búsqueda con grandes cantidades
de nodos, como mapas o rejillas de nodos (grids). Es muy utilizado en
problemas de búsqueda de rutas, por ejemplo, en videojuegos y sistemas
de navegación.

\begin{itemize}
\item \textbf{Conceptos Básicos}
\label{sec:orgccf1be2}

A* combina las ventajas de los algoritmos de búsqueda de costo
uniforme (como Dijkstra) y búsqueda heurística (como la búsqueda en
anchura) para obtener un algoritmo que es tanto \textbf{\textbf{óptimo}} (encuentra
la mejor solución) como \textbf{\textbf{completable}} (lo hace en un tiempo
razonable).

A* utiliza una función de evaluación que combina dos valores clave:
\begin{itemize}
\item \textbf{\textbf{g(n)}}: El costo real desde el nodo inicial hasta el nodo actual \(n\).
\item \textbf{\textbf{h(n)}}: Una estimación heurística del costo desde el nodo actual \(n\) hasta el nodo final (objetivo).
\end{itemize}

La función de evaluación se define como:
\begin{minted}[]{latex}
f(n) = g(n) + h(n)
\end{minted}
donde \(g(n)\) es el costo acumulado y \(h(n)\) es la estimación
heurística. La idea es expandir primero los nodos que parecen estar
más cerca del objetivo, mientras se asegura que el costo acumulado sea
mínimo.

\textbf{Características del Algoritmo A*}
\begin{itemize}
\item \textbf{Óptimo}: Si la heurística es admisible, es decir, nunca sobreestima
el costo restante (por ejemplo, si se usa la \textbf{distancia Manhattan}
o la \textbf{distancia Euclidiana} como heurística en un espacio
euclidiano), A* siempre encontrará el camino más corto.
\item \textbf{Completo}: Si existe un camino desde el punto inicial al punto
objetivo, A* lo encontrará, siempre que el espacio de búsqueda sea
finito.
\end{itemize}

\textbf{Etapas del Algoritmo}

\begin{enumerate}
\item \textbf{Inicialización}:
\begin{itemize}
\item Se parte de un nodo inicial (el punto de partida) y se agrega a una estructura de datos (generalmente una \textbf{\textbf{cola de prioridad}}).
\item Cada nodo tiene un valor de \(f(n)\) calculado como la suma de \(g(n)\) y \(h(n)\), donde \(g(n)\) es 0 para el nodo inicial y \(h(n)\) es el valor heurístico estimado.
\end{itemize}

\item \textbf{\textbf{Expansión de Nodos}}:
\begin{itemize}
\item En cada paso, el algoritmo extrae el nodo con el menor valor \(f(n)\) de la cola de prioridad.
\item Luego, el algoritmo \textbf{\textbf{expande}} este nodo, es decir, explora todos sus nodos vecinos.
\item Para cada vecino, se calcula el nuevo costo \(g(n)\) como la distancia acumulada desde el nodo inicial y se actualiza el valor de \(f(n)\).
\end{itemize}

\item \textbf{\textbf{Actualización de Nodos Vecinos}}:
\begin{itemize}
\item Si un vecino aún no ha sido explorado, o si se ha encontrado un camino más corto a dicho vecino, se actualizan sus valores de \(g(n)\) y \(f(n)\), y se agrega a la cola de prioridad para ser evaluado más adelante.
\end{itemize}

\item \textbf{\textbf{Heurística}}:
\begin{itemize}
\item La heurística \(h(n)\) es crucial para guiar la búsqueda hacia el objetivo. Una \textbf{\textbf{buena heurística}} hace que A* sea más eficiente, ya que prioriza la expansión de los nodos más prometedores.
\item Las heurísticas comunes incluyen la \textbf{\textbf{distancia Manhattan}} (para cuadrículas donde solo se puede mover en horizontal o vertical) y la \textbf{\textbf{distancia Euclidiana}} (para espacios donde se puede mover en diagonal).
\end{itemize}

\item \textbf{\textbf{Terminación}}:
\begin{itemize}
\item El algoritmo termina cuando se extrae el nodo objetivo (el nodo final) de la cola de prioridad, lo que significa que se ha encontrado el camino más corto.
\end{itemize}

\item \textbf{\textbf{Reconstrucción del Camino}}:
\begin{itemize}
\item Una vez que se ha encontrado el nodo final, el camino se reconstruye retrocediendo desde el nodo final al nodo inicial a través de los nodos predecesores.
\end{itemize}
\end{enumerate}

\item \textbf{Pseudocódigo}
\label{sec:orgb2552e5}

Pseudocódigo simplificado del algoritmo A*:

\begin{minted}[]{python}
Function A*(inicio, objetivo)
    crear una cola de prioridad `open_set`
    agregar `inicio` a `open_set` con f(inicio) = h(inicio)
    
    g_score[inicio] = 0
    f_score[inicio] = h(inicio)  // Heurística desde inicio hasta objetivo

    While `open_set` no esté vacío:
        current = nodo en `open_set` con menor valor de f_score
        If current == objetivo:
            return reconstruir_camino(current)  // Camino encontrado

        quitar current de `open_set`

        For cada vecino de current:
            tentative_g_score = g_score[current] + costo para moverse a vecino
            
            If tentative_g_score < g_score[vecino]:  // Mejor camino encontrado
                came_from[vecino] = current
                g_score[vecino] = tentative_g_score
                f_score[vecino] = g_score[vecino] + h(vecino, objetivo)
                
                If vecino no está en `open_set`:
                    agregar vecino a `open_set`

    Return fracaso  // Si no se encuentra un camino
\end{minted}

\textbf{Ejemplo de Heurística: Distancia Manhattan}

Si se está trabajando en una cuadrícula donde solo se permiten movimientos horizontales y verticales, la \textbf{\textbf{distancia Manhattan}} es una heurística común:

\(h(n) = |x1 - x2| + |y1 - y2|\)

donde \((x1, y1)\) son las coordenadas del nodo actual \(n\) y \((x2, y2)\) son las coordenadas del nodo objetivo.

\textbf{Complejidad del Algoritmo}

\begin{itemize}
\item \textbf{Complejidad espacial}: A* puede consumir bastante memoria, ya que mantiene una lista de nodos abiertos y cerrados (nodos que han sido evaluados).
\item \textbf{Complejidad temporal}: En el peor de los casos, A* tiene una complejidad de tiempo de \(O(b^d)\), donde \(b\) es el factor de ramificación (número promedio de vecinos por nodo) y \(d\) es la profundidad del camino más corto.
\end{itemize}

\textbf{Ventajas de A*}

\begin{enumerate}
\item \textbf{Eficiente}: Encuentra el camino más corto de manera eficiente si la heurística es adecuada.
\item \textbf{Óptimo}: Siempre encuentra el camino más corto si la heurística es admisible.
\item \textbf{Flexible}: Funciona en muchos tipos de grafos y mapas.
\end{enumerate}

\textbf{Desventajas de A*}
\begin{enumerate}
\item \textbf{Costoso en memoria}: Puede requerir mucho espacio, especialmente en problemas de gran escala.
\item \textbf{Dependencia de la heurística}: Si la heurística no es adecuada, el rendimiento de A* puede degradarse significativamente.
\end{enumerate}

\textbf{Aplicaciones}

\begin{itemize}
\item \textbf{Videojuegos}: Usado en motores de IA para encontrar rutas en mapas grandes, por ejemplo, en juegos de estrategia.
\item \textbf{Sistemas de navegación}: GPS y otros sistemas de navegación utilizan variantes de A* para encontrar rutas óptimas.
\item \textbf{Robótica}: En sistemas de planificación de rutas para robots autónomos.
\end{itemize}


\textbf{Visualización de Nodos, Cascaron A*} 

\begin{minted}[]{python}
import pygame

# Configuraciones iniciales
ANCHO_VENTANA = 800
VENTANA = pygame.display.set_mode((ANCHO_VENTANA, ANCHO_VENTANA))
pygame.display.set_caption("Visualización de Nodos")

# Colores (RGB)
BLANCO = (255, 255, 255)
NEGRO = (0, 0, 0)
GRIS = (128, 128, 128)
VERDE = (0, 255, 0)
ROJO = (255, 0, 0)
NARANJA = (255, 165, 0)
PURPURA = (128, 0, 128)

class Nodo:
    def __init__(self, fila, col, ancho, total_filas):
        self.fila = fila
        self.col = col
        self.x = fila * ancho
        self.y = col * ancho
        self.color = BLANCO
        self.ancho = ancho
        self.total_filas = total_filas

    def get_pos(self):
        return self.fila, self.col

    def es_pared(self):
        return self.color == NEGRO

    def es_inicio(self):
        return self.color == NARANJA

    def es_fin(self):
        return self.color == PURPURA

    def restablecer(self):
        self.color = BLANCO

    def hacer_inicio(self):
        self.color = NARANJA

    def hacer_pared(self):
        self.color = NEGRO

    def hacer_fin(self):
        self.color = PURPURA

    def dibujar(self, ventana):
        pygame.draw.rect(ventana, self.color, (self.x, self.y, self.ancho, self.ancho))

def crear_grid(filas, ancho):
    grid = []
    ancho_nodo = ancho // filas
    for i in range(filas):
        grid.append([])
        for j in range(filas):
            nodo = Nodo(i, j, ancho_nodo, filas)
            grid[i].append(nodo)
    return grid

def dibujar_grid(ventana, filas, ancho):
    ancho_nodo = ancho // filas
    for i in range(filas):
        pygame.draw.line(ventana, GRIS, (0, i * ancho_nodo), (ancho, i * ancho_nodo))
        for j in range(filas):
            pygame.draw.line(ventana, GRIS, (j * ancho_nodo, 0), (j * ancho_nodo, ancho))

def dibujar(ventana, grid, filas, ancho):
    ventana.fill(BLANCO)
    for fila in grid:
        for nodo in fila:
            nodo.dibujar(ventana)

    dibujar_grid(ventana, filas, ancho)
    pygame.display.update()

def obtener_click_pos(pos, filas, ancho):
    ancho_nodo = ancho // filas
    y, x = pos
    fila = y // ancho_nodo
    col = x // ancho_nodo
    return fila, col

def main(ventana, ancho):
    FILAS = 10
    grid = crear_grid(FILAS, ancho)

    inicio = None
    fin = None

    corriendo = True

    while corriendo:
        dibujar(ventana, grid, FILAS, ancho)
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                corriendo = False

            if pygame.mouse.get_pressed()[0]:  # Click izquierdo
                pos = pygame.mouse.get_pos()
                fila, col = obtener_click_pos(pos, FILAS, ancho)
                nodo = grid[fila][col]
                if not inicio and nodo != fin:
                    inicio = nodo
                    inicio.hacer_inicio()

                elif not fin and nodo != inicio:
                    fin = nodo
                    fin.hacer_fin()

                elif nodo != fin and nodo != inicio:
                    nodo.hacer_pared()

            elif pygame.mouse.get_pressed()[2]:  # Click derecho
                pos = pygame.mouse.get_pos()
                fila, col = obtener_click_pos(pos, FILAS, ancho)
                nodo = grid[fila][col]
                nodo.restablecer()
                if nodo == inicio:
                    inicio = None
                elif nodo == fin:
                    fin = None

    pygame.quit()

main(VENTANA, ANCHO_VENTANA)

\end{minted}


\item \textbf{Tutorial de Pygame: Visualización de Nodos, Cascaron A*}
\label{sec:orga6b7aaa}

Este programa utiliza la biblioteca Pygame para crear una cuadrícula
interactiva donde puedes definir un nodo de inicio, un nodo final, y
establecer paredes. Es una base para implementar algoritmos como A*
para la búsqueda de caminos.

\begin{itemize}
\item 1. Importar Pygame
\label{sec:org9f07536}

El primer paso es importar Pygame, que es una biblioteca para la creación de videojuegos en Python:

\begin{minted}[]{python}
import pygame
\end{minted}

\item 2. Configuraciones iniciales
\label{sec:orgf4ba852}
Se configuran la ventana y los colores que se usarán en la
cuadrícula. `ANCHO\textsubscript{VENTANA}` define el tamaño de la ventana, y
`VENTANA` se usa para crear la ventana donde se dibujarán los
elementos:

\begin{minted}[]{python}
ANCHO_VENTANA = 800
VENTANA = pygame.display.set_mode((ANCHO_VENTANA, ANCHO_VENTANA))
pygame.display.set_caption("Visualización de Nodos")
\end{minted}

\begin{itemize}
\item \textbf{`ANCHO\textsubscript{VENTANA}`}: Define que la ventana será de 800x800 píxeles.
\item \textbf{`pygame.display.set\textsubscript{mode}`}: Crea la ventana con las dimensiones dadas.
\item \textbf{`pygame.display.set\textsubscript{caption}`}: Establece el título de la ventana.
\end{itemize}

\item 3. Definir colores en formato RGB
\label{sec:org7801549}
Definimos algunos colores que se usarán para los nodos y las paredes. Los colores se expresan en el formato RGB:

\begin{minted}[]{python}
BLANCO = (255, 255, 255)
NEGRO = (0, 0, 0)
GRIS = (128, 128, 128)
VERDE = (0, 255, 0)
ROJO = (255, 0, 0)
NARANJA = (255, 165, 0)
PURPURA = (128, 0, 128)
\end{minted}

\item 4. Clase Nodo
\label{sec:org8ff1333}
La clase `Nodo` representa cada celda de la cuadrícula. Cada nodo
tiene una posición (fila y columna), un color, y puede tener
diferentes estados (inicio, fin, pared, etc.):

\begin{minted}[]{python}
class Nodo:
    def __init__(self, fila, col, ancho, total_filas):
        self.fila = fila
        self.col = col
        self.x = fila * ancho
        self.y = col * ancho
        self.color = BLANCO
        self.ancho = ancho
        self.total_filas = total_filas
\end{minted}

Esta clase contiene varios métodos útiles:
\begin{itemize}
\item \textbf{`get\textsubscript{pos}()`}: Devuelve la posición del nodo.
\item \textbf{`es\textsubscript{pared}()`, `es\textsubscript{inicio}()`, `es\textsubscript{fin}()`}: Determinan el estado del nodo.
\item \textbf{`hacer\textsubscript{pared}()`, `hacer\textsubscript{inicio}()`, `hacer\textsubscript{fin}()`}: Cambian el estado del nodo.
\item \textbf{`dibujar()`}: Dibuja el nodo en la ventana.
\end{itemize}

\item 5. Crear la cuadrícula
\label{sec:org73d1bc0}
El método `crear\textsubscript{grid}` crea una lista de nodos organizados en una
cuadrícula. Cada nodo tiene un tamaño calculado dividiendo el ancho
total de la ventana por el número de filas.

\begin{minted}[]{python}
def crear_grid(filas, ancho):
    grid = []
    ancho_nodo = ancho // filas
    for i in range(filas):
        grid.append([])
        for j in range(filas):
            nodo = Nodo(i, j, ancho_nodo, filas)
            grid[i].append(nodo)
    return grid
\end{minted}

Este método crea una lista bidimensional que representa la cuadrícula de nodos.

\item 6. Dibujar la cuadrícula y los nodos
\label{sec:orgf906cbf}
El método `dibujar\textsubscript{grid}` dibuja las líneas que separan los nodos en la cuadrícula, mientras que `dibujar` dibuja cada nodo en la ventana.

\begin{minted}[]{python}
def dibujar_grid(ventana, filas, ancho):
    ancho_nodo = ancho // filas
    for i in range(filas):
        pygame.draw.line(ventana, GRIS, (0, i * ancho_nodo), (ancho, i * ancho_nodo))
        for j in range(filas):
            pygame.draw.line(ventana, GRIS, (j * ancho_nodo, 0), (j * ancho_nodo, ancho))
\end{minted}

\begin{itemize}
\item \textbf{`dibujar\textsubscript{grid}`}: Dibuja las líneas horizontales y verticales para formar la cuadrícula.
\item \textbf{`dibujar`}: Llama al método `dibujar()` de cada nodo para mostrar su estado (color).
\end{itemize}

\item 7. Obtener la posición del clic
\label{sec:org58197e2}
El método `obtener\textsubscript{click}\textsubscript{pos}` convierte la posición en píxeles del ratón a la posición de la cuadrícula (fila y columna).

\begin{minted}[]{python}
def obtener_click_pos(pos, filas, ancho):
    ancho_nodo = ancho // filas
    y, x = pos
    fila = y // ancho_nodo
    col = x // ancho_nodo
    return fila, col
\end{minted}

Esto permite identificar qué nodo fue clicado con precisión.

\item 8. Ciclo principal
\label{sec:orga638b88}
El ciclo principal del programa está en la función `main`, que
controla la interacción del usuario. Escucha eventos como el cierre de
la ventana o clics del ratón para definir los nodos de inicio, fin, y
las paredes.

\begin{minted}[]{python}
def main(ventana, ancho):
    FILAS = 10
    grid = crear_grid(FILAS, ancho)

    inicio = None
    fin = None

    corriendo = True

    while corriendo:
        dibujar(ventana, grid, FILAS, ancho)
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                corriendo = False

            if pygame.mouse.get_pressed()[0]:  # Click izquierdo
                pos = pygame.mouse.get_pos()
                fila, col = obtener_click_pos(pos, FILAS, ancho)
                nodo = grid[fila][col]
                if not inicio and nodo != fin:
                    inicio = nodo
                    inicio.hacer_inicio()

                elif not fin and nodo != inicio:
                    fin = nodo
                    fin.hacer_fin()

                elif nodo != fin and nodo != inicio:
                    nodo.hacer_pared()

            elif pygame.mouse.get_pressed()[2]:  # Click derecho
                pos = pygame.mouse.get_pos()
                fila, col = obtener_click_pos(pos, FILAS, ancho)
                nodo = grid[fila][col]
                nodo.restablecer()
                if nodo == inicio:
                    inicio = None
                elif nodo == fin:
                    fin = None

    pygame.quit()
\end{minted}

\begin{itemize}
\item \textbf{Interacción del ratón}:
\begin{itemize}
\item \textbf{Clic izquierdo}: Define el nodo de inicio, el nodo final, o establece una pared.
\item \textbf{Clic derecho}: Restablece el nodo (lo borra).
\end{itemize}
\end{itemize}

Finalmente, la función `pygame.quit()` se encarga de cerrar correctamente el programa.
\end{itemize}



\item Fuentes sobre el Algoritmo A*
\label{sec:org71f4ea9}

\begin{enumerate}
\item \href{https://en.wikipedia.org/wiki/A*\_search\_algorithm}{Wikipedia: A* Search Algorithm}
\item \href{https://theory.stanford.edu/\~amitp/GameProgramming/AStarComparison.html}{Stanford University: Introduction to A*}
\item \href{https://aquariusai.ca/a-star-algorithm/}{Aquarius AI: A* Algorithm in AI}
\item \href{https://brilliant.org/wiki/a-star-search/}{Brilliant Math \& Science: A* Search Algorithm}
\end{enumerate}
\end{itemize}


\subsubsection*{Algoritmos de búsqueda local.}
\label{sec:org65f7543}






\section*{Espacio de estados}
\label{sec:orgfd5c8ed}

Muchos de los problemas que pueden ser resueltos aplicando técnicas de
inteligencia artificial se modelan en forma simbólica y discreta
definiendo las configuraciones posibles del universo estudiado.  El
problema se plantea entoces en términos de encontrar una configuración
objetivo a partir de una configuración inicial dada, aplicando
transformaciones válidas según el modelo del universo.  La respuesta
es la secuencia de transformaciones cuya aplicación succesiva lleva a
la configuración deseada.

Los ejemplos más carácteristicos de esta categoría de problemas son
los juegos (son universos restringidos fáciles de modelar). En un
juego, las configuraciones del universo corresponden directamente a
las configuraciones del tablero. Cada configuración es un estado que
puede ser esquematizado gráficamente y representado en forma
simbólica. Las transformaciones permitidas corresponden a las reglas o
movidas del juego, formalizadas como transiciones de estado.

Entonces, para plantear formalmente un problema, se requiere precisar
una representación simbólica de los estados y definir reglas del tipo
condición acción para cada una de las transiciones válidas dentro del
universo modelado. La acción de una regla indica como modificar el
estado actual para generar un nuevo estado.  La condición impone
restricciones sobre la aplicabilidad de la regla según el estado
actual, el estado generado o la historia completa del proceso de
solución.

El espacio de estados de un juego es un grafo cuyos nodos representan
las configuraciones alcanzables (los estados válidos) y cuyos arcos
explicitan las movidas posibles (las transiciones de estado).  En
principio, se puede construir cualquier espacio de estados partiendo
del estado inicial, aplicando cada una de las reglas para generar los
sucesores immediatos, y así succesivamente con cada uno de los nuevos
estados generados (en la práctica, los espacios de estados suelen ser
demasiado grandes para explicitarlos por completo).

Cuando un problema se puede representar mediante un espacio de
estados, la solución computacional correspende a encontrar un camino
desde el estado inicial a un estado objetivo.

\subsection*{Ejemplo de espacio de estados}
\label{sec:orge75d2a6}

\subsubsection*{Descripción del problema}
\label{sec:orgf806f03}

Un arriero se encuentra en el borde de un rio llevando un puma, una
cabra y una lechuga.  Debe cruzar a la otra orilla por medio de un
bote con capacidad para dos (el arriero y alguna de sus
pertenecias). La dificultad es que si el puma se queda solo con la
cabra la devorará, y lo mismo sucederá si la cabra se queda sola con
la lechuga. ¿Cómo cruzar sin perder ninguna pertenencia?

\subsubsection*{Representación de las configuraciones del universo del problema:}
\label{sec:org11ad11e}

Basta precisar la situación antes o después de cruzar. El arriero y
cada una de sus pertenencias tienen que estar en alguna de las dos
orillas. La representación del estado debe entonces indicar en que
lado se encuentra cada uno de ellos. Para esto se puede utilizar un
término simbólico con la siguiente sintáxis: estado(A,P,C,L), en que
A, P, C y L son variables que representan, respectivamente, la
posición del arriero, el puma, la cabra y la lechuga. Las variables
pueden tomar dos valores: i y d, que simbolizan respectivamente el
borde izquierdo y el borde derecho del rio. Por convención se elige
partir en el borde izquierdo. El estado inicial es entonces
estado(i,i,i,i). El estado objetivo es estado(d,d,d,d).

\subsubsection*{Definición de las reglas de transición:}
\label{sec:org561de22}
El arriero tiene cuatro acciones posibles: cruzar solo, cruzar con el
puma, cruzar con la cabra y cruzar con la lechuga. Estas acciones
están condicionadas a que ambos pasajeros del bote estén en la misma
orilla y a que no queden solos el puma con la cabra o la cabra con la
lechuga. El estado resultante de una acción se determina
intercambiando los valores i y d para los pasajeros del bote.

\subsubsection*{Generación del espacio de estados}
\label{sec:orgf9bcc2d}
En este ejemplo se puede explicitar todo el espacio de estados (el
número de configuraciones está acotado por 24).

\begin{center}
\includegraphics[width=.9\linewidth]{img/sd.png}
\end{center}



\subsubsection*{Problemas de los Canibales y Monjes}
\label{sec:orgac3f9cd}

Se tienen 3 monjes y 3 caníbales en el margen Oeste de un río. Existe
una canoa con capacidad para dos personas como máximo. Se desea que
los seis pasen al margen Este del río, pero hay que considerar que no
debe haber más caníbales que monjes en ningún sitio porque entonces
los caníbales se comen a los monjes. Además, la canoa siempre debe ser
conducida por alguien.\\


\subsubsection*{El espacio de estados está definido por}
\label{sec:org3e5d2f6}

\{(Mo, Co, Me, Ce, C) / Mo es el número de monjes en el margen oeste con
0<=Mo<=3\\
 AND Co es el número de caníbales en el margen oeste con 0<=Co<=3
AND (Co<=Mo OR Mo=0)\\
 AND Me es el número de monjes en el margen este con
0<=Me<=3 \\
AND Ce es el número de caníbales en el margen este con 0<=Ce<=3
AND (Ce<=Me OR Me=0) AND Co+Ce=3 AND Mo+Me=3 AND C = [E|O] es el margen dónde está la canoa\}\\

El estado inicial es (3,3,0,0,O)

El estado final es (0,0,3,3,E)

Las reglas que se pueden aplicar son:

\begin{itemize}
\item Viajan un monje y un caníbal de O a E:
Si (Mo, Co, Me, Ce, O) AND Mo>=1 AND Co>=1 AND Ce+1<=Me+1 => (Mo-1, Co-1, Me+1, Ce+1, E)
\item Viajan un monje y un caníbal de E a O:
Si (Mo, Co, Me, Ce, E) AND Me>=1 AND Ce>=1 AND Co+1<=Mo+1=> (Mo+1, Co+1, Me-1, Ce-1,O)

\item Viajan dos monjes de O a E:
Si (Mo, Co, Me, Ce, O) AND Mo>=2 AND (Mo-2=0 OR Co<=Mo-2) AND Ce<=Me+2=> (Mo-2, Co, Me+2, Ce, E)

\item Viajan dos monjes de E a O:
Si (Mo, Co, Me, Ce, E) AND Me>=2 AND (Me-2=0 OR Ce<=Me-2) AND Co<=Mo+2 => (Mo+2, Co, Me-2, Ce, O)

\item Viajan dos caníbales de O a E:
Si (Mo, Co, Me, Ce, O) AND Co>=2 AND (Me=0 OR Ce+2<=Me) => (Mo, Co-2, Me, Ce+2, E)

\item Viajan dos caníbales de E a O:
Si (Mo, Co, Me, Ce, E) AND Ce>=2 AND (Mo=0 OR Co+2<=Mo) => (Mo, Co+2, Me, Ce-2, O)

\item Viaja un monje de O a E:
Si (Mo, Co, Me, Ce, O) AND Mo>=1 AND (Mo-1=0 OR Co<=Mo-1) AND Ce<= Me+1 => (Mo-1, Co, Me+1, Ce, E)

\item Viaja un monje de E a O:
Si (Mo, Co, Me, Ce, E) AND Me>=1 AND (Me-1=0 OR Ce<=Me-1) AND Co<=Mo+1 => (Mo+1, Co, Me-1, Ce,O)

\item Viaja un caníbal de O a E:
Si (Mo, Co, Me, Ce, O) AND Co>=1 AND (Me=0 OR Ce+1<=Me) => (Mo, Co-1, Me, Ce+1, E)

\item Viaja un caníbal de E a O:
Si (Mo, Co, Me, Ce, O) AND Ce>=1 AND (Mo=0 OR Co+1<=Mo) => (Mo, Co+1, Me, Ce-1, E)
\end{itemize}


Nota: En referencia a la regla 3 la condición Ce<=Me+2 puede intuirse
como redundante. Esta condición no se cumple sólo en el caso Ce=3 y
Me=0. Pese a

que es un estado que pertenece al espacio de estados válidos, podemos
intuir que nunca se llega a tener 3 caníbales y ningún monje del lado
Este y la barca del lado Oeste. De todas maneras sólo se puede
eliminar si podemos demostrar formalmente la imposibilidad de esta
situación.

Un pasaje de estados para ir de (3,3,0,0,O) a (0,0,3,3,E) es el siguiente:

(3,3,0,0,O) => (3,1,0,2,E) => (3,2,0,1,O) => (3,0,0,3,E) => (3,1,0,2,O) =>
(1,1,2,2,E) => (2,2,1,1,O) => (0,2,3,1,E) => (0,3,3,0,O) => (0,1,3,2,E) =>
(0,2,3,1,O) =>(0,0,3,3,E)

\subsection*{Representación de espacio de estados}
\label{sec:org8aa7ea0}
La primera pregunta es, como  

\subsection*{El problema del n-Puzzle}
\label{sec:org01ca3d5}

\subsubsection*{Caracterización de las búsquedas ciegas.}
\label{sec:orga989e44}
La búsqueda ciega o no informada sólo utiliza información acerca de si
un estado es o no objetivo para guiar su procesu de búsqueda.

Los métodos de búsqueda ciega se pueden clasificar en dos grupos
básicos:

\begin{itemize}
\item \textbf{Métodos de búsqueda en anchura}: Son procedimientos de búsqueda nivel
a nivel. Para cada uno de los nodos de un nivel se aplican todos los
posibles operadores y no se expande ningún nodo de un nivel antes de
haber expandido todos los del nivel anterior.

\item \textbf{Métodos de búsqueda en profundidad}: En estos procedimientos se realiza la búsqueda por
una sola rama del árbol hasta encontrar una solución o hasta que se tome la decisión de
terminar la búsqueda por esa dirección ( por no haber posibles operadores que aplicar sobre
el nodo hoja o por haber alcanzado un nivel de profundidad muy grande ) . Si esto ocurre
se produce una vuelta atrás ( backtracking ) y se sigue por otra rama hasta visitar todas
las ramas del árbol si es necesario.
\end{itemize}


A partir de los dos tipos de búsqueda anteriores surgió uno nuevo,
llamado método de búsqueda por profundización iterativa. El algoritmo
de búsqueda más representativo de esta nueva tendencia es el DFID
acrónimo de su nombre en inglés (Depth-First Iterative-Deepening).


\subsubsection*{Caracterización de las búsquedas heurísticas.}
\label{sec:org1e8e379}

Las técnicas de búsqueda heurística se apoyan alc contrario de los
métodos de búsqueda ciega se apoyan en información adicional para
realizar su proceso de búsqueda. Para mejorar la eficiencia de la
búsqueda, estos algoritmos hacen uso de una función que realiza una
predicción del coste necesario para alcanzar la solución. La función
que guía el proceso toma el nombre de función heurística.

De todos los algoritmos de búsqueda heurística, uno destaca en
especial: el A*. Este algoritmo, a pesar de haber sido creado entorno
a los años 60, sigue en la actualidad siendo uno de los mas
utilizados. Desafortunadamente, es ineficiente en cuanto al uso de
memoria durante el proceso de búsqueda. Por ello, en las décadas de
los 80 y 90, aparecieron algoritmos basados en el propio A*, pero que
limitaban el uso de memoria. Dos de los algoritmos más representativos
de esta última tendencia son el IDA* (Iterative-Deepening A*) y el
SMA* (Simplified Memory-bounded A*).

\section*{Técnicas de Búsqueda}
\label{sec:org7b82fbf}

\subsection*{Solución de problemas con búsqueda.}
\label{sec:org064ce57}


La solución de problemas es fundamental para la mayoría de las
aplicaciones de IA; existen principalmente dos clases de problemas que
se pueden resolver mediante procesos computables: aquéllos en los que
se utiliza un algoritmo determinista que garantiza la solución al
problema y las tareas complejas que se resuelven con la búsqueda de
una solución; de ésta última clase de problemas se ocupa la IA.

La solución de problemas requiere dos consideraciones:

\begin{itemize}
\item Representación del problema en un espacio organizado.
\item La capacidad de probar la existencia del estado objetivo en dicho espacio.
\end{itemize}

Las anteriores premisas se traducen en: la determinación del estado
objetivo y la determinación del camino óptimo guiado por este objetivo
a través de una o más transiciones dado un estado inicial

El espacio de búsqueda, se le conoce como una colección de estados.
En general los espacios de búsqueda en los problemas de IA no son completamente conocidos de forma a priori.
De lo anterior ‘resolver un problema de IA’ cuenta con dos fases:$\backslash$\[0.5cm]

\begin{itemize}
\item La generación del espacio de estados
\item La búsqueda del estado deseado en ese espacio.
\end{itemize}

Debido a que "todo el espacio de búsqueda" de un problema es muy
grande, puede causar un bloqueo de memoria, dejando muy poco espacio
para el proceso de búsqueda. Para solucionar esto, se expande el
espacio paso a paso, hasta encontrar el estado objetivo.


\subsection*{Espacios de Estados}
\label{sec:orgc285d97}

Muchos de los problemas que pueden ser resueltos aplicando técnicas de inteligencia artificial se modelan en forma simbólica y
discreta definiendo las configuraciones posibles del universo estudiado. El problema se plantea entonces en términos de encontrar una configuración objetivo a partir de una configuración inicial dada, aplicando transformaciones válidas según el modelo del universo. La respuesta es la secuencia de transformaciones cuya aplicación succesiva lleva a la configuración deseada.
Los ejemplos más carácteristicos de esta categoría de problemas son los juegos (son universos restringidos fáciles de modelar). En un juego, las configuraciones del universo corresponden directamente a las configuraciones del tablero. Cada configuración es un estado que puede ser esquematizado gráficamente y representado en forma simbólica. Las transformaciones permitidas corresponden a las reglas o movidas del juego, formalizadas como transiciones de estado.
Entonces, para plantear formalmente un problema, se requiere precisar una representación simbólica de los estados y definir reglas del tipo condición   acción para cada una de las transiciones válidas dentro del universo modelado. La acción de una regla indica como modificar el estado actual para generar un nuevo estado. La condición impone restricciones sobre la aplicabilidad de la regla según el estado actual, el estado generado o la historia completa del proceso de solución.
El espacio de estados de un juego es un grafo cuyos nodos representan las configuraciones alcanzables (los estados válidos) y cuyos arcos explicitan las movidas posibles (las transiciones de estado). En principio, se puede construir cualquier espacio de estados partiendo del estado inicial, aplicando cada una de las reglas para generar los sucesores immediatos, y así succesivamente con cada uno de los nuevos estados generados (en la práctica, los espacios de estados suelen ser demasiado grandes para explicitarlos por completo).
Cuando un problema se puede representar mediante un espacio de estados, la solución computacional correspende a encontrar un camino desde el estado inicial a un estado objetivo.

\subsubsection*{Deterministicos}
\label{sec:org52674de}

El espacio de estados determinísticos contienen un único estado inicial y seguir la secuencia de estados para la solución. Los espacios de estados determinísticos son usados por los sistemas expertos.
Se puede describir asu vez, que un sistema es determinístico si, para un estado dado, al menos aplica una regla a él y de solo una manera.

\subsubsection*{No Deterministicos}
\label{sec:org0e5577c}
El no determinístico contiene un amplio número de estados iniciales y sigue la secuencia de estados perteneciente al estado inicial del espacio. Son usados por sistemas de lógica difusa.
En otras palabras,  si más de una regla aplica a cualquier estado particular del sistema, o si una regla aplica a un estado particular del sistema en más de una manera, entonces el sistema es no determinístico.


\subsection*{Métodos de Búsqueda}
\label{sec:org7b6bd66}

\subsubsection*{Primero en anchura (breadthfirst)}
\label{sec:org2e43712}
En inglés, breadth-first search.
Si el conjunto open se maneja como una lista FIFO, es decir, como una cola, siempre se estará visitando primero los primeros estados en ser generados. El recorrido del espacio de estados se hace por niveles de profundidad.

\begin{minted}[]{c}
procedure Busqueda_en_amplitud {
   open ()[estado_inicial]
   closed () {}
   while (open no esta vacia) {
     remover el primer estado X de la lista open
     if (X es un estado objetivo) return exito
     else {
       generar el conjunto de sucesores del estado X
       agregar el estado X al conjunto closed
       eliminar sucesores que ya estan en open o en closed
       agregar el resto de los sucesores al final de open
     }
   }
   return fracaso
 }

\end{minted}

Si el factor de ramificación es B y la profundidad a la cual se encuentra el estado objetivo más cercano es n, este algoritmo tiene una complejidad en tiempo y espacio de \(O(B^n)\).
Contrariamente a la búsqueda en profundidad, la búsqueda en amplitud garantiza encontrar el camino más corto.  

\subsubsection*{Primero en profundidad (depthfirst).}
\label{sec:org6b5aced}

En inglés, depth-first search.
Si el conjunto open se maneja como una lista LIFO, es decir, como un stack, siempre se estará
visitando primero los últimos estados en ser generados. Esto significa que si A genera B y C, y B
genera D, antes de visitar C se visita D, que está más alejado de la raiz A, o sea más profundo en
el árbol de búsqueda. El algoritmo tiene en este caso la tendencia de profundizar la búsqueda en
una rama antes de explorar ramas alternativas.

\begin{minted}[]{c}
procedure Busqueda_en_profundidad {
   open () [estado_inicial]
   closed () {}
   while (open no esta vacia) {
     remover el primer estado X de la lista open
     if (X es un estado objetivo) return exito
     else {
       generar el conjunto de sucesores del estado X
       agregar el estado X al conjunto closed
       eliminar sucesores que ya estan en open o en closed
       agregar el resto de los sucesores al principio de open
     }
   }
   return fracaso
 }


\end{minted}

Considerando que la cantidad promedio de sucesores de los nodos visitados es B (llamado en inglés el
branching factor y en castellano el factor de ramificación), y suponiendo que la profundidad máxima alcanzada es n,
este algoritmo tiene una complejidad en tiempo de \(O(B^n)\) y, si no se considera el conjunto closed, una complejidad en
espacio de O(B × n). En vez de usar el conjunto closed, el control de ciclos se puede hacer descartando aquellos estados que aparecen en el camino generado hasta el momento (basta que cada estado generado tenga un puntero a su padre).
El mayor problema de este algoritmo es que puede "perderse" en una rama sin encontrar la solución. Además, si se encuentra una solución no se puede garantizar que sea el camino más corto.

\subsubsection*{Búsqueda Heurística}
\label{sec:orgb004d2d}
El algoritmo de búsqueda A* (pronunciado "A asterisco", "A estrella" o
"Astar" en inglés) se clasifica dentro de los algoritmos de búsqueda
en grafos de tipo heurístico o informado. Presentado por primera vez
en 1968 por Peter E. Hart, Nils J. Nilsson y Bertram Raphael, el
algoritmo A* encuentra, siempre y cuando se cumplan unas determinadas
condiciones, el camino de menor coste entre un nodo origen y uno
objetivo.\\

El problema de algunos algoritmos de búsqueda en grafos informados,
como puede ser el algoritmo voraz, es que se guían en exclusiva por la
función heurística, la cual puede no indicar el camino de coste más
bajo, o por el coste real de desplazarse de un nodo a otro (como los
algoritmos de escalada), pudiéndose dar el caso de que sea necesario
realizar un movimiento de coste mayor para alcanzar la solución. Es
por ello bastante intuitivo el hecho de que un buen algoritmo de
búsqueda informada debería tener en cuenta ambos factores, el valor
heurístico de los nodos y el coste real del recorrido.

Así, el algoritmo A* utiliza una función de evaluación
\(f(n)=g(n)+h'(n)\), donde \(h'(n)\) representa el valor heurístico del
nodo a evaluar desde el actual, n, hasta el final, y \(g(n)\) \(g(n)\), el
coste real del camino recorrido para llegar a dicho nodo, n, desde el
nodo inicial. A* mantiene dos estructuras de datos auxiliares, que
podemos denominar abiertos, implementado como una cola de prioridad
(ordenada por el valor \(f(n)\) de cada nodo), y cerrados, donde se
guarda la información de los nodos que ya han sido visitados. En cada
paso del algoritmo, se expande el nodo que esté primero en abiertos, y
en caso de que no sea un nodo objetivo, calcula la \(f(n)\) de todos sus
hijos, los inserta en abiertos, y pasa el nodo evaluado a cerrados.

El algoritmo es una combinación entre búsquedas del tipo primero en
anchura con primero en profundidad: mientras que \(h'(n)\) tiende a
primero en profundidad, \(g(n)\) tiende a primero en anchura. De este
modo, se cambia de camino de búsqueda cada vez que existen nodos más
prometedores.



\begin{itemize}
\item Propiedades
\label{sec:org97d918d}
Como todo algoritmo de búsqueda en amplitud, A* es un algoritmo
completo: en caso de existir una solución, siempre dará con ella.

Si para todo nodo n del grafo se cumple \(g(n)=0\), nos encontramos ante
una búsqueda voraz. Si para todo nodo n del grafo se cumple \(h(n)=0\),
A* se comporta como el algoritmo de Dijkstra.

Para garantizar la admisibilidad del algoritmo, la función \(h(n)\)
debe ser heurística admisible, esto es, que no sobrestime el coste
real de alcanzar el nodo objetivo, es decir, h(n) debe ser menor que
h*(n) para todo nodo no final.

Se garantiza que \(h(n)\) es
consistente (o monótona), es decir, que para cualquier nodo
\(n\) y cualquiera de sus sucesores, el coste estimado de
alcanzar el objetivo desde n no es mayor que el de alcanzar el sucesor
más el coste de alcanzar el objetivo desde el sucesor.



\item Complejidad
\label{sec:orga3fdc1b}

La complejidad computacional del algoritmo está íntimamente
relacionada con la calidad de la heurística que se utilice en el
problema. En el caso peor, con una heurística de pésima calidad, la
complejidad será exponencial, mientras que en el caso mejor, con una
buena \(h'(n)\), el algoritmo se
ejecutará en tiempo lineal. Para que esto último suceda, se debe
cumplir que

$$ h'(x)\leq g(y)-g(x)+h'(y)$$ donde h' es una heurística óptima para el problema,
como por ejemplo, el coste real de alcanzar el objetivo.


El espacio requerido por A* para ser ejecutado es su mayor
problema. Dado que tiene que almacenar todos los posibles siguientes
nodos de cada estado, la cantidad de memoria que requerirá será
exponencial con respecto al tamaño del problema. Para solucionar este
problema, se han propuesto diversas variaciones de este algoritmo,
como pueden ser RTA*, IDA* o SMA*.
\end{itemize}

\subsection*{Satisfacción de restricciones.}
\label{sec:org264e7b3}
Los problemas pueden resolverse buscando en un espacio de estados, estos estados pueden evaluarse por heurísticas específicas para el dominio y probados para verificar si son estados meta.
Los componentes del estado, son equivalentes a un grafo de restricciones, los cuales están compuestos de:$\backslash$\[0.5cm]

\begin{itemize}
\item \textbf{Variables}: Dominios (valores posibles para las variables).
\item \textbf{Restricciones} (binarias) entre las variables.
\end{itemize}


Objetivo: encontrar un estado (una asignación completa de valores a las variables) Que satisface las restricciones.

En los Problemas de Satisfacción de Restricciones (PSR), los estados y
la prueba de meta siguen a una representación estándar, estructurada y
muy simple.

Ejemplos:

\begin{itemize}
\item Crucigramas
\item Colorear mapas
\end{itemize}

\section*{Teoría de juegos.}
\label{sec:org098a963}

Siendo una de las principales capacidades de la inteligencia humana su
capacidad para resolver problemas, así como la habilidad para analizar
los elementos esenciales de cada problema, abstrayéndolos, el
identificar las acciones que son necesarias para resolverlos y el
determinar cuál es la estrategia más acertada para atacarlos, son
rasgos fundamentales.

Podemos definir la resolución de problemas como el proceso que
partiendo de unos datos iníciales y utilizando un conjunto de
procedimientos escogidos, es capaz de determinar el conjunto de pasos
o elementos que nos llevan a lo que denominaremos una solución óptima
o semi-óptima de un problema de planificación, descubrir una
estrategia ganadora de un juego, demostrar un teorema, reconocer

Una imagen, comprender una oración o un texto son algunas de las
tareas que pueden concebirse como de resolución.

Una gran ventaja que nos proporciona la utilización de los juegos es
que a través de ellos es muy fácil medir el éxito o el fracaso, por lo
que podemos comprobar si las técnicas y algoritmos empleados son los
óptimos. En comparación con otras aplicaciones de inteligencia
artificial, por ejemplo comprensión del lenguaje, los juegos no
necesitan grandes cantidades de algoritmos. Los juegos más utilizados
son las damas y el ajedrez.



\section*{Grafos}
\label{sec:org79e0230}

Un grafo es un conjunto de puntos (vértices) en el espacio, que están conectados
por un conjunto de líneas (aristas). Otros conceptos básicos son:
Dos vértices son adyacentes si comparten la misma arista.
Los extremos de una arista son los vértices que comparte dicha arista.
Un grafo se dice que es finito si su número de vértices es finito.

\section*{Tipos de grafos}
\label{sec:orga7d09d8}

Existen dos tipos de grafos los no dirigidos y los dirigidos.

• \textbf{No dirigidos}: son aquellos en los cuales los lados no están orientados (No son
flechas). Cada lado se representa entre paréntesis, separando sus vértices por
comas, y teniendo en cuenta (Vi,Vj)=(Vj,Vi).

• \textbf{Dirigidos}: son aquellos en los cuales los lados están orientados (flechas).
Cada lado se representa entre ángulos, separando sus vértices por comas y
teniendo en cuenta <Vi ,Vj>!=<Vj ,Vi>. En grafos dirigidos, para cada lado <A,B>,
A, el cual es el vértice origen, se conoce como la cola del lado y B, el cual es
el vértice destino, se conoce como cabeza del lado.

\section*{Machine Learning}
\label{sec:orgf9ed9fd}


El aprendizaje automático o aprendizaje automatizado o aprendizaje de
máquinas (del inglés, machine learning) es el subcampo de las ciencias
de la computación y una rama de la inteligencia artificial, cuyo
objetivo es desarrollar técnicas que permitan que las computadoras
aprendan. Se dice que un agente aprende cuando su desempeño mejora con
la experiencia y mediante el uso de datos; es decir, cuando la
habilidad no estaba presente en su genotipo o rasgos de nacimiento.1​
"En el aprendizaje de máquinas un computador observa datos, construye
un modelo basado en esos datos y utiliza ese modelo a la vez como una
hipótesis acerca del mundo y una pieza de software que puede resolver
problemas".

En muchas ocasiones el campo de actuación del aprendizaje automático
se solapa con el de la estadística inferencial, ya que las dos
disciplinas se basan en el análisis de datos. Sin embargo, el
aprendizaje automático incorpora las preocupaciones de la complejidad
computacional de los problemas. Muchos problemas son de clase NP-hard,
por lo que gran parte de la investigación realizada en aprendizaje
automático está enfocada al diseño de soluciones factibles a esos
problemas. El aprendizaje automático también está estrechamente
relacionado con el reconocimiento de patrones. El aprendizaje
automático puede ser visto como un intento de automatizar algunas
partes del método científico mediante métodos matemáticos. Por lo
tanto es un proceso de inducción del conocimiento.

El aprendizaje automático tiene una amplia gama de aplicaciones,
incluyendo motores de búsqueda, diagnósticos médicos, detección de
fraude en el uso de tarjetas de crédito, análisis del mercado de
valores, clasificación de secuencias de ADN, reconocimiento del habla
y del lenguaje escrito, juegos y robótica.

\subsection*{Tipos de Algoritmos}
\label{sec:org57296ce}

Los diferentes algoritmos de Aprendizaje Automático se agrupan en una
taxonomía en función de la salida de los mismos. Algunos tipos de
algoritmos son:

\begin{itemize}
\item \textbf{Aprendizaje supervisado} : El algoritmo produce una función que
establece una correspondencia entre las entradas y las salidas
deseadas del sistema. Un ejemplo de este tipo de algoritmo es el
problema de clasificación, donde el sistema de aprendizaje trata de
etiquetar (clasificar) una serie de vectores utilizando una entre
varias categorías (clases). La base de conocimiento del sistema está
formada por ejemplos de etiquetados anteriores. Este tipo de
aprendizaje puede llegar a ser muy útil en problemas de
investigación biológica, biología computacional y bioinformática.

\item \textbf{Aprendizaje no supervisado}: Todo el proceso de modelado se lleva a
cabo sobre un conjunto de ejemplos formado tan solo por entradas al
sistema. No se tiene información sobre las categorías de esos
ejemplos. Por lo tanto, en este caso, el sistema tiene que ser capaz
de reconocer patrones para poder etiquetar las nuevas entradas.

\item \textbf{Aprendizaje semisupervisado}: Este tipo de algoritmos combinan los
dos algoritmos anteriores para poder clasificar de manera
adecuada. Se tiene en cuenta los datos marcados y los no marcados.

\item \textbf{Aprendizaje por refuerzo}: El algoritmo aprende observando el mundo
que le rodea. Su información de entrada es el feedback o
retroalimentación que obtiene del mundo exterior como respuesta a
sus acciones. Por lo tanto, el sistema aprende a base de
ensayo-error. El aprendizaje por refuerzo es el más general entre
las tres categorías. En vez de que un instructor indique al agente
qué hacer, el agente inteligente debe aprender cómo se comporta el
entorno mediante recompensas (refuerzos) o castigos, derivados del
éxito o del fracaso respectivamente. El objetivo principal es
aprender la función de valor que le ayude al agente inteligente a
maximizar la señal de recompensa y así optimizar sus políticas de
modo a comprender el comportamiento del entorno y a tomar buenas
decisiones para el logro de sus objetivos formales.  Los principales
algoritmos de aprendizaje por refuerzo se desarrollan dentro de los
métodos de resolución de problemas de decisión finitos de Markov,
que incorporan las ecuaciones de Bellman y las funciones de
valor. Los tres métodos principales son: la Programación Dinámica,
los métodos de Monte Carlo y el aprendizaje de Diferencias
Temporales. Entre las implementaciones desarrolladas está AlphaGo,
un programa de IA desarrollado por Google DeepMind para jugar el
juego de mesa Go. En marzo de 2016 AlphaGo le ganó una partida al
jugador profesional Lee Se-Dol que tiene la categoría noveno dan y
18 títulos mundiales. Entre los algoritmos que utiliza se encuentra
el árbol de búsqueda Monte Carlo, también utiliza aprendizaje
profundo con redes neuronales. Puede ver lo ocurrido en el
documental de Netflix “AlphaGo”.

\item \textbf{Transducción}: Similar al aprendizaje supervisado, pero no construye
de forma explícita una función. Trata de predecir las categorías de
los futuros ejemplos basándose en los ejemplos de entrada, sus
respectivas categorías y los ejemplos nuevos al sistema.

\item \textbf{Aprendizaje multi-tarea}: Métodos de aprendizaje que usan
conocimiento previamente aprendido por el sistema de cara a
enfrentarse a problemas parecidos a los ya vistos. El análisis
computacional y de rendimiento de los algoritmos de aprendizaje
automático es una rama de la estadística conocida como teoría
computacional del aprendizaje. El aprendizaje automático las
personas lo llevamos a cabo de manera
automática ya que es un proceso tan sencillo para nosotros que ni nos
damos cuenta de cómo se realiza y todo lo que implica. Desde que
nacemos hasta que morimos los seres humanos llevamos a cabo diferentes
procesos, entre ellos encontramos el de aprendizaje por medio del cual
adquirimos conocimientos, desarrollamos habilidades para analizar y
evaluar a través de métodos y técnicas así como también por medio de
la experiencia propia. Sin embargo, a las máquinas hay que indicarles
cómo aprender, ya que si no se logra que una máquina sea capaz de
desarrollar sus habilidades, el proceso de aprendizaje no se estará
llevando a cabo, sino que solo será una secuencia repetitiva.
\end{itemize}

\subsection*{Técnicas de clasificación}
\label{sec:orgc2b2447}

\begin{itemize}
\item \textbf{Árboles de decisiones}: Este tipo de aprendizaje usa un árbol de
decisiones como modelo predictivo. Se mapean observaciones sobre un
objeto con conclusiones sobre el valor final de dicho objeto. Los
árboles son estructuras básicas en la informática. Los árboles de
atributos son la base de las decisiones. Una de las dos formas
principales de árboles de decisiones es la desarrollada por Quinlan
de medir la impureza de la entropía en cada rama, algo que primero
desarrolló en el algoritmo ID3 y luego en el C4.5. Otra de las
estrategias se basa en el índice GINI y fue desarrollada por
Breiman, Friedman et alia. El algoritmo de CART es una
implementación de esta estrategia.5​

\item \textbf{Reglas de asociación}: Los algoritmos de reglas de asociación
procuran descubrir relaciones interesantes entre variables. Entre
los métodos más conocidos se hallan el algoritmo a priori, el
algoritmo Eclat y el algoritmo de Patrón Frecuente.

\item \textbf{Algoritmos genéticos}: Los algoritmos genéticos son procesos de
búsqueda heurística que simulan la selección natural. Usan métodos
tales como la mutación y el cruzamiento para generar nuevas clases
que puedan ofrecer una buena solución a un problema dado.

\item \textbf{Redes neuronales artificiales}: Las redes de neuronas artificiales
(RNA) son un paradigma de aprendizaje automático inspirado en las
neuronas de los sistemas nerviosos de los animales. Se trata de un
sistema de enlaces de neuronas que colaboran entre sí para producir
un estímulo de salida. Las conexiones tienen pesos numéricos que se
adaptan según la experiencia. De esta manera, las redes neurales se
adaptan a un impulso y son capaces de aprender. La importancia de
las redes neurales cayó durante un tiempo con el desarrollo de los
vectores de soporte y clasificadores lineales, pero volvió a surgir
a finales de la década de 2000 con la llegada del aprendizaje
profundo.

\item \textbf{Máquinas de vectores de soporte}: Las MVS son una serie de métodos
de aprendizaje supervisado usados para clasificación y
regresión. Los algoritmos de MVS usan un conjunto de ejemplos de
entrenamiento clasificado en dos categorías para construir un modelo
que prediga si un nuevo ejemplo pertenece a una u otra de dichas
categorías.

\item \textbf{Algoritmos de agrupamiento} El análisis por agrupamiento
(clustering en inglés) es la clasificación de observaciones en
subgrupos —clusters— para que las observaciones en cada grupo se
asemejen entre sí según ciertos criterios. Las técnicas de
agrupamiento hacen inferencias diferentes sobre la estructura de los
datos; se guían usualmente por una medida de similitud específica y
por un nivel de compactamiento interno (similitud entre los miembros
de un grupo) y la separación entre los diferentes grupos.

El agrupamiento es un método de aprendizaje no supervisado y es una
técnica muy popular de análisis estadístico de datos.

\item \textbf{Redes bayesianas} Una red bayesiana, red de creencia o modelo
acíclico dirigido es un modelo probabilístico que representa una
serie de variables de azar y sus independencias condicionales a
través de un grafo acíclico dirigido. Una red bayesiana puede
representar, por ejemplo, las relaciones probabilísticas entre
enfermedades y síntomas. Dados ciertos síntomas, la red puede usarse
para calcular las probabilidades de que ciertas enfermedades estén
presentes en un organismo. Hay algoritmos eficientes que infieren y
aprenden usando este tipo de representación.
\end{itemize}


\subsection*{Clasificador en Cascada}
\label{sec:orgedfbede}

La detección de objetos usando un clasificador en cascada basado en
características de Haar es un método efectivo de detección de objetos
propuesto en 2001 por Paul Viola y Michael Jones en su artículo
"Detección rápida de objetos usando cascada mejorada de
características simples". Este es un método basado en el aprendizaje
automático en el que se entrena una función en cascada a partir de
muchas imágenes positivas y negativas. Luego se usa para detectar
objetos en otras imágenes.

El algoritmo requiere una gran cantidad de imágenes positivas
(imágenes faciales) e imágenes negativas (no imágenes faciales) para
entrenar al clasificador. Luego, necesitamos extraer características
de él. Para hacer esto, use la función Haar que se muestra en la
siguiente figura. Son como nuestros núcleos de convolución. Cada
característica es un valor único que se obtiene restando la suma de
píxeles debajo del rectángulo blanco de la suma de píxeles debajo del
rectángulo negro.

\begin{center}
\includegraphics[width=.9\linewidth]{img/har.png}
\end{center}

Ahora, todos los tamaños y posiciones posibles de cada kernel se
utilizan para calcular muchas funciones. (Imagínese cuántos cálculos
genera. Incluso una ventana de 24x24 generará más de 160.000
características). Para el cálculo de cada característica, necesitamos
encontrar la suma de los píxeles debajo de los rectángulos blanco y
negro. Para resolver este problema, introdujeron la imagen general. No
importa qué tan grande sea su imagen, reducirá el cálculo de un píxel
dado a operaciones que involucren solo cuatro píxeles.

Pero de todas estas características que calculamos, la mayoría de
ellas no son relevantes. Por ejemplo, considere la siguiente
figura. La primera fila muestra dos buenas características. La primera
característica elegida pareció centrarse en la naturaleza del área de
los ojos, que generalmente es más oscura que las áreas de la nariz y
las mejillas. La segunda característica elegida se basa en que las
propiedades de los ojos son más oscuras que el puente de la nariz. Sin
embargo, aplicar la misma ventana en las mejillas o en cualquier otro
lugar es irrelevante. Entonces, ¿cómo elegimos la mejor función entre
más de 160.000 funciones?.

\begin{center}
\includegraphics[width=.9\linewidth]{img/har2.png}
\end{center}


Para ello, aplicamos todas las funciones a todas las imágenes de
entrenamiento. Para cada característica, encontrará el mejor umbral,
que divide el rostro en positivo y negativo. Obviamente, habrá errores
o clasificaciones erróneas. Elegimos las características con la tasa
de error más baja, lo que significa que son las características más
precisas para clasificar imágenes faciales y no faciales. (Este
proceso no es tan simple. Al principio, el peso de cada imagen es
igual. Después de cada clasificación, el peso de la imagen mal
clasificada aumentará. Luego se realizará el mismo proceso. Se
calculará la nueva tasa de error. También calcular Peso
nuevo. Continúe con este proceso hasta que se alcance la precisión o
la tasa de error requeridas o se encuentre el número requerido de
funciones.

El clasificador final es la suma ponderada de estos clasificadores
débiles. Se denomina clasificación débil porque no puede clasificar
imágenes por sí sola, sino que forma un clasificador fuerte junto con
otras clasificaciones. El documento dice que incluso 200 funciones
pueden proporcionar una detección de precisión del 95\%. Su
configuración final tiene aproximadamente 6000 funciones. (Imagínese
reducir de más de 160.000 funciones a 6.000 funciones. Esto es una
gran ganancia).

Entonces ahora toma una foto. Toma cada ventana de 24x24. Aplicarle
6000 funciones. Busque caras. Vaya \ldots{} ¿No es esto ineficiente y
requiere mucho tiempo? Si. El autor tiene una buena solución para
esto.

En la imagen, la mayoría de las imágenes son áreas sin caras. Por lo
tanto, es mejor tener una manera fácil de verificar si la ventana no
es un área frontal. Si no es así, deséchelo todo de una vez y no lo
vuelva a procesar. En cambio, concéntrese en las áreas que pueden
tener caras. De esta forma, dedicaremos más tiempo a comprobar
posibles zonas faciales.

Con este fin, introdujeron el concepto de clasificadores en
cascada. En lugar de aplicar los 6000 componentes funcionales a una
ventana, estos componentes funcionales se agrupan en diferentes etapas
de clasificación y se aplican uno por uno. (Por lo general, las
primeras etapas contendrán muy pocas funciones). Si la ventana falla
en la primera etapa, se descarta. No consideramos sus funciones
restantes. Si pasa, se aplica la segunda etapa de la función y el
proceso continúa. La ventana a través de todas las etapas es un área
facial. ¡Qué tal este plan!

El detector del autor tiene más de 6000 características con 38 etapas,
con 1, 10, 25, 25 y 50 características en las primeras cinco
etapas. (Las dos funciones en la imagen de arriba son en realidad las
dos mejores funciones obtenidas de Adaboost). Según el autor, cada
subventana evaluó un promedio de 10 características de más de 6000
características.

Por lo tanto, esta es una explicación simple e intuitiva del principio
de funcionamiento de la detección de rostros Viola-Jones. Lea este
artículo para obtener más detalles o consulte las referencias en la
sección de otros recursos.

\subsubsection*{Ejemplo de clasificación utilizando Haarcascades}
\label{sec:org6430ee1}

\begin{itemize}
\item \href{https://github.com/opencv/opencv/tree/master/data/haarcascades}{Clasificadores Haarcascades de la librería Opencv}
\item \href{https://opencv-python-tutroals.readthedocs.io/en/latest/py\_tutorials/py\_objdetect/py\_face\_detection/py\_face\_detection.html}{Tutorial Haarcascades}
\item \href{https://docs.opencv.org/2.4/doc/user\_guide/ug\_traincascade.html}{Entrenamiento Haarcascades}
\end{itemize}


\begin{minted}[]{python}
import numpy as np
import cv2 as cv

rostro = cv.CascadeClassifier('haarcascade_frontalface_alt.xml')
cap = cv.VideoCapture(0)
x=y=w=h= 0 
img = 0
count = 0
while True:
    ret, frame = cap.read()
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    rostros = rostro.detectMultiScale(gray, 1.3, 5)
    for(x, y, w, h) in rostros:
        m= int(h/2)
        frame = cv.rectangle(frame, (x,y), (x+w, y+h), (0, 255, 0), 2)
        frame = cv.rectangle(frame, (x,y+m), (x+w, y+h), (255, 0 ,0), 2 )
        img = 180- frame[y:y+h,x:x+w]
        count = count + 1   
    
    #name = '/home/likcos/imgs/cara'+str(count)+'.jpg'
    #cv.imwrite(name, frame)
    cv.imshow('rostros', frame)
    cv.imshow('cara', img)
    
    k = cv.waitKey(1)
    if k == 27:
        break
cap.release()
cv.destroyAllWindows()
\end{minted}

\subsection*{Segmentación de Color}
\label{sec:org6dee490}

La segmentación de imágenes es un tema ampliamente estudiado para la
extracción y reconocimiento de objetos, de acuerdo a las
características de textura, color, forma, entre otros. Dependiendo de
la naturaleza del problema, las características de color de los
objetos pueden proporcionar información relevante sobre ellos. Por
ejemplo, la segmentación de imágenes de color ha sido aplicado en
diferentes áreas como análisis de alimentos, geología,
medicina entre otras.  Los trabajos que abordan la
segmentación de imágenes por características de color emplean
diferentes técnicas, pero las más empleadas son las redes
neuronales (RN) y métodos basado en agrupamiento,
específicamente, fuzzy c-means (FCM). Las RN son entrenadas
para reconocer colores específicos, es decir, estas son entrenadas con
los colores de la imagen a ser segmentada. Si se da una nueva imagen
la RN debe ser entrenada nuevamente. Al emplear métodos basados en
agrupamiento, se crean grupos de colores con características
similares. La desventaja con tales métodos es que se requiere definir
previamente la cantidad de grupos en que se divide la información; por
lo tanto, el número de grupos se define dependiendo de la naturaleza
de la escena.  Nuestra propuesta consiste en entrenar a la RN para
reconocer diferentes colores, tratando de emular la percepción humana
del color. Los seres humanos identifican principalmente los colores
por su cromaticidad, después por su intensidad [21]. Por ejemplo, si
se le pregunta a cualquier persona cual es el color de los cuadros (a)
y (b) de la Fig. 1, lo más seguro es que responderá “verde”; nótese
que el cuadro (a) es más brilloso que el cuadro (b) pero la
cromaticidad no cambia. Ahora, si se le vuelve a preguntar a esa misma
persona cual es el color de los cuadros (c) y (d) de la Fig. 1, lo más
seguro es que responda “rojo y rosa, respectivamente”; es importante
mencionar que los cuadros (c) y (d) tienen la misma intensidad pero
diferentes cromaticidades.


\subsection*{Árboles de decisión}
\label{sec:org0715c18}

Los árboles de decisión (DT) son un método de aprendizaje supervisado
no paramétrico que se utiliza para la clasificación y la regresión. El
objetivo es crear un modelo que prediga el valor de una variable de
destino mediante el aprendizaje de reglas de decisión simples
deducidas de las características de los datos. Un árbol puede verse
como una aproximación constante por partes.

en el siguiente ejemplo, los árboles de decisión aprenden de los datos
para aproximarse a una curva sinusoidal con un conjunto de reglas de
decisión if-then-else. Cuanto más profundo es el árbol, más complejas
son las reglas de decisión y más ajustado es el modelo

\begin{center}
\includegraphics[width=.9\linewidth]{img/dtr.png}
\end{center}


\subsubsection*{Elementos}
\label{sec:org9054a13}

Los árboles de decisión están formados por nodos, vectores de números,
flechas y etiquetas.

\begin{itemize}
\item Cada nodo se puede definir como el momento en el que se ha de tomar
una decisión de entre varias posibles, lo que va haciendo que a
medida que aumenta el número de nodos aumente el número de posibles
finales a los que puede llegar el individuo. Esto hace que un árbol
con muchos nodos sea complicado de dibujar a mano y de analizar
debido a la existencia de numerosos caminos que se pueden seguir.
\item Los vectores de números serían la solución final a la que se llega
en función de las diversas posibilidades que se tienen, dan las
utilidades en esa solución.
\item Las flechas son las uniones entre un nodo y otro y representan cada
acción distinta.
\item Las etiquetas se encuentran en cada nodo y cada flecha y dan nombre
a cada acción.
\end{itemize}

\subsubsection*{Algunas ventajas de los árboles de decisión son:}
\label{sec:orgb26aedd}

\begin{itemize}
\item Fácil de entender y de interpretar. Los árboles se pueden
visualizar.

\item Requiere poca preparación de datos. Otras técnicas a menudo
requieren la normalización de datos, es necesario crear variables
ficticias y eliminar valores en blanco. Sin embargo, tenga en cuenta
que este módulo no admite valores faltantes.

\item El costo de usar el árbol (es decir, predecir datos) es logarítmico
en la cantidad de puntos de datos usados ​​para entrenar el árbol.
\end{itemize}
\begin{itemize}
\item Capaz de manejar datos numéricos y categóricos. Otras técnicas
suelen estar especializadas en analizar conjuntos de datos que
tienen un solo tipo de variable.

\item Capaz de manejar problemas de múltiples salidas.

\item Utiliza un modelo de caja blanca. Si una situación dada es
observable en un modelo, la explicación de la condición se explica
fácilmente mediante lógica booleana. Por el contrario, en un modelo
de caja negra (por ejemplo, en una red neuronal artificial), los
resultados pueden ser más difíciles de interpretar.

\item Posibilidad de validar un modelo mediante pruebas estadísticas. Eso
permite dar cuenta de la fiabilidad del modelo.

\item Tiene un buen desempeño incluso si sus supuestos son algo violados
por el verdadero modelo a partir del cual se generaron los dato
\end{itemize}

\subsubsection*{Desventajas de los árboles de decisión:}
\label{sec:org9cf28ba}

\begin{itemize}
\item El aprendizaje de los  árboles de decisión pueden crear árboles demasiado
complejos que no generalizan bien los datos. Esto se llama
sobreajuste. Para evitar este problema, son necesarios mecanismos
como la poda, establecer el número mínimo de muestras requeridas en
un nudo de la hoja o establecer la profundidad máxima del árbol.

\item Los árboles de decisión pueden ser inestables porque pequeñas
variaciones en los datos pueden generar un árbol completamente
diferente. Este problema se mitiga mediante el uso de árboles de
decisión dentro de un conjunto.

\item Las predicciones de los árboles de decisión no son uniformes ni
continuas, sino aproximaciones constantes por partes, como se ve en
la figura anterior. Por lo tanto, no son buenos para la
extrapolación.

\item Se sabe que el problema de aprender un árbol de decisión óptimo es
NP-completo bajo varios aspectos de optimización e incluso para
conceptos simples. En consecuencia, los algoritmos prácticos de
aprendizaje del árbol de decisiones se basan en algoritmos
heurísticos, como el algoritmo voraz, en el que se toman decisiones
localmente óptimas en cada nodo. Dichos algoritmos no pueden
garantizar la devolución del árbol de decisión globalmente
óptimo. Esto se puede mitigar entrenando varios árboles en un alumno
de conjunto, donde las características y las muestras se muestrean
aleatoriamente con reemplazo.

\item Hay conceptos que son difíciles de aprender porque los árboles de
decisión no los expresan fácilmente, como XOR, paridad o problemas
de multiplexor.

\item Los aprendices de árboles de decisión crean árboles sesgados si
dominan algunas clases. Por lo tanto, se recomienda equilibrar el
conjunto de datos antes de ajustarlo al árbol de decisión.
\end{itemize}

\subsubsection*{Ejemplo de Clasificación, Árbol de decisión scikit-learn}
\label{sec:org28b55a1}

\textbf{DecisionTreeClassifier} es una clase capaz de realizar una
clasificación de varias clases en un conjunto de datos.

Al igual que con otros clasificadores, \textbf{DecisionTreeClassifier} toma
como entrada dos matrices: una matriz \textbf{X}, dispersa o densa, de forma
(n\textsubscript{muestras}, n\textsubscript{características}) que contiene las muestras de
entrenamiento, y una matriz \textbf{Y} de valores enteros, forma (n\textsubscript{muestras}),
que contiene las etiquetas de clase. para las muestras de
entrenamiento:

\begin{minted}[]{python}
from sklearn import tree
X = [[0, 0], [1, 1]]
Y = [0, 1]
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)
clf.predict([[2.,2.]]) 
\end{minted}


Despues de ajustarce el modelo se puede usar, para predecir la clase 
\begin{minted}[]{python}
clf.predict([[2.,2.]]) 
#array([1])
\end{minted}


En caso de que haya múltiples clases con la misma y mayor
probabilidad, el clasificador predecirá la clase con el índice más
bajo entre esas clases.

Como alternativa a generar una clase específica, se puede predecir la
probabilidad de cada clase, que es la fracción de muestras de
entrenamiento de la clase en una hoja:

\begin{minted}[]{python}
clf.predict_proba([[2., 2.]])
#array([[0., 1.]])
\end{minted}


DecisionTreeClassifier es capaz tanto de clasificación binaria (donde
las etiquetas son [-1, 1]) como de clasificación multiclase (donde las
etiquetas son [0, …, K-1]).

Usando el conjunto de datos de Iris, podemos construir un árbol de la
siguiente manera:

\begin{minted}[]{python}
# Importar las bibliotecas necesarias
from sklearn.datasets import load_iris
from sklearn import tree
import graphviz

# Cargar el conjunto de datos Iris
iris = load_iris()

X, y = iris.data, iris.target
print( X, y)
# Crear el clasificador del Árbol de Decisión
clf = tree.DecisionTreeClassifier()

# Entrenar el modelo con los datos
clf = clf.fit(X, y)

# Exportar el árbol de decisión en formato DOT para su visualización
dot_data = tree.export_graphviz(clf, out_file=None, 
                                feature_names=iris.feature_names,  
                                class_names=iris.target_names,  
                                filled=True, rounded=True,  
                                special_characters=True)  

# Crear el gráfico con graphviz
graph = graphviz.Source(dot_data)

# Guardar el gráfico como un archivo PDF (opcional)
graph.render("iris_decision_tree")

# Mostrar el gráfico directamente
graph.view()

\end{minted}

\subsubsection*{Árbol de Decisión en el Conjunto de Datos Iris}
\label{sec:org1cd52b3}

\begin{itemize}
\item Estructura del Árbol de Decisión
\label{sec:org432c492}
Un árbol de decisión tiene la siguiente estructura:
\begin{itemize}
\item \textbf{\textbf{Raíz del árbol}}: El nodo inicial donde comienza la primera división.
\item \textbf{\textbf{Nodos internos}}: Nodos que dividen los datos en función de una característica.
\item \textbf{\textbf{Hojas}}: Nodos finales donde se realiza la clasificación. No hay más divisiones posibles en estos nodos.
\end{itemize}

\item Nodo raíz
\label{sec:orgf7720e9}
La primera decisión se toma en base a \textbf{\textbf{petal width (cm) ≤ 0.8}}:
\begin{itemize}
\item \textbf{\textbf{gini = 0.667}}: El índice de Gini mide la impureza del nodo. Un
valor de 0 significa pureza total (todas las muestras pertenecen a
una clase), mientras que un valor de 0.667 indica mezcla.
\item \textbf{\textbf{samples = 150}}: Hay 150 muestras en este nodo.
\item \textbf{\textbf{value = [50, 50, 50]}}: Hay 50 flores de cada clase (\textbf{\textbf{setosa}}, \textbf{\textbf{versicolor}}, \textbf{\textbf{virginica}}).
\item \textbf{\textbf{class = setosa}}: Aunque el nodo está mezclado, la clase mayoritaria es \textbf{\textbf{setosa}}.
\end{itemize}

\item Primera división
\label{sec:org0fa0c8e}
\begin{itemize}
\item \textbf{\textbf{Si petal width (cm) ≤ 0.8}}:
\begin{itemize}
\item \textbf{\textbf{gini = 0.0}}, \textbf{\textbf{samples = 50}}, \textbf{\textbf{value = [50, 0, 0]}}. Todas las flores en este nodo son de la clase \textbf{\textbf{setosa}}, por lo que el modelo puede clasificarlas sin más divisiones.
\end{itemize}

\item \textbf{\textbf{Si petal width (cm) > 0.8}}:
\begin{itemize}
\item El modelo sigue dividiendo los datos de las otras dos clases (\textbf{\textbf{versicolor}} y \textbf{\textbf{virginica}}).
\end{itemize}
\end{itemize}

\item Segunda división
\label{sec:org9320af2}
Para el resto de las flores, el modelo utiliza la característica \textbf{\textbf{petal width (cm) ≤ 1.75}}:
\begin{itemize}
\item \textbf{\textbf{gini = 0.5}}, \textbf{\textbf{samples = 100}}, \textbf{\textbf{value = [0, 50, 50]}}. Aquí hay una mezcla equilibrada de las clases \textbf{\textbf{versicolor}} y \textbf{\textbf{virginica}}.
\item Si el ancho del pétalo es menor o igual a 1.75 cm, el modelo predice \textbf{\textbf{versicolor}}, mientras que si es mayor, predice \textbf{\textbf{virginica}}.
\end{itemize}

\item Divisiones más profundas
\label{sec:orgcf9eeca}
El árbol sigue dividiendo basándose en otras características, como \textbf{\textbf{petal length (cm) ≤ 4.95}}:
\begin{itemize}
\item Si esta condición se cumple, el modelo predice \textbf{\textbf{versicolor}}. Si no, comienza a clasificar flores como \textbf{\textbf{virginica}}.
\end{itemize}

\item Hojas finales
\label{sec:org3b29f1f}
Cada nodo hoja finaliza la clasificación:
\begin{itemize}
\item Si un nodo tiene \textbf{\textbf{gini = 0.0}}, significa que todas las muestras en ese nodo pertenecen a una sola clase.
\item Por ejemplo, en un nodo final con \textbf{\textbf{gini = 0.0}}, \textbf{\textbf{samples = 43}}, \textbf{\textbf{value = [0, 0, 43]}}, todas las flores en ese nodo son de la clase \textbf{\textbf{virginica}}.
\end{itemize}
\end{itemize}

\subsubsection*{Índice de Gini en Árboles de Decisión}
\label{sec:orgd2b5538}

\begin{itemize}
\item ¿Qué es el índice de Gini?
\label{sec:org15b8d8c}
El \textbf{\textbf{índice de Gini}} es una medida de impureza o diversidad que se
utiliza en algoritmos de aprendizaje automático, como los Árboles de
Decisión. Se utiliza para medir qué tan puras o mezcladas están las
clases en un nodo del árbol.

\item Concepto del Índice de Gini
\label{sec:org8bebd5f}
El índice de Gini mide la probabilidad de que una muestra elegida al
azar sea clasificada incorrectamente si se la asigna una clase de
manera aleatoria basada en la distribución de clases en ese nodo.

\begin{itemize}
\item \textbf{\textbf{Un nodo puro}} tendrá un índice de Gini igual a 0, lo que significa
que todas las muestras en ese nodo pertenecen a la misma clase.
\item \textbf{\textbf{Un nodo impuro}} tendrá un índice de Gini mayor, lo que indica que
hay una mezcla de clases en ese nodo.
\end{itemize}

\item Fórmula del Índice de Gini
\label{sec:org8cfac02}
El índice de Gini se calcula como:

\[
Gini = 1 - \sum_{i=1}^{n} p_i^2
\]

Donde:
\begin{itemize}
\item \textbf{\textbf{n}} es el número de clases.
\item \textbf{\textbf{p\textsubscript{i}}} es la proporción de ejemplos de la clase \textbf{\textbf{i}} en el nodo.
\end{itemize}

\item Ejemplos del Índice de Gini
\label{sec:org27d80a7}

\begin{itemize}
\item Ejemplo 1: Nodo completamente puro
\label{sec:org9eeadc0}
Si todas las muestras en un nodo pertenecen a una sola clase, entonces el índice de Gini es \textbf{\textbf{0}}.

\begin{itemize}
\item Ejemplo: Si hay 100 muestras y todas pertenecen a la clase "A", entonces \(p_A = 1\) y las demás probabilidades son 0.
\item Cálculo:
\end{itemize}
\begin{minted}[]{python}
Gini = 1 - (1^2) = 0
\end{minted}
Esto indica que el nodo es completamente puro.

\item Ejemplo 2: Nodo con clases mixtas
\label{sec:org485e3cc}
Si las muestras están distribuidas equitativamente entre varias clases, el índice de Gini será mayor.

\begin{itemize}
\item Ejemplo: Si hay 100 muestras, con 50 de la clase "A" y 50 de la clase "B", entonces \(p_A = 0.5\) y \(p_B = 0.5\).
\item Cálculo:
\end{itemize}
\begin{minted}[]{python}
Gini = 1 - (0.5^2 + 0.5^2) = 1 - 0.25 - 0.25 = 0.5
\end{minted}
Esto indica que el nodo contiene una mezcla de clases.

\item Ejemplo 3: Nodo con clases desbalanceadas
\label{sec:org55a033c}
Si en un nodo hay 90 muestras de la clase "A" y 10 de la clase "B", el índice de Gini será más bajo que en el caso de clases equilibradas.

\begin{itemize}
\item Ejemplo: \(p_A = 0.9\) y \(p_B = 0.1\).
\item Cálculo:
\end{itemize}
\begin{minted}[]{python}
Gini = 1 - (0.9^2 + 0.1^2) = 1 - 0.81 - 0.01 = 0.18
\end{minted}
Esto indica que la mayoría de las muestras pertenecen a una clase, pero el nodo aún no es completamente puro.
\end{itemize}

\item Interpretación del Índice de Gini
\label{sec:org72b8bab}
\begin{itemize}
\item \textbf{\textbf{Gini = 0}}: El nodo es completamente puro (todas las muestras pertenecen a una sola clase).
\item \textbf{\textbf{Gini cercano a 0}}: La mayoría de las muestras pertenecen a una clase, pero hay algo de mezcla.
\item \textbf{\textbf{Gini cercano a 1}}: El nodo es muy impuro, con una mezcla casi uniforme de varias clases.
\end{itemize}

\item Uso en Árboles de Decisión
\label{sec:orgdbc1fc4}
El árbol de decisión selecciona la característica que minimiza el índice de Gini en los nodos hijos, creando divisiones que generen nodos lo más puros posibles.

\item Ejemplo del Índice de Gini en tu Árbol de Decisión
\label{sec:org01d939d}
En el nodo raíz del árbol de decisión que generaste, el \textbf{\textbf{índice de Gini}} es 0.667:
\begin{itemize}
\item Esto indica que hay una mezcla significativa de clases en ese nodo.
\item El árbol de decisión divide los datos para reducir el índice de Gini en los nodos hijos.
\end{itemize}
\end{itemize}

\subsubsection*{Ejemplo con Dataset Phaser}
\label{sec:org6729980}

\begin{minted}[]{python}
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, export_graphviz
import graphviz

# Cargar el dataset
file_path = 'phaser.csv'
dataset = pd.read_csv(file_path)

# Eliminar columnas innecesarias (como la vacía "Unnamed: 3")
#dataset = dataset.drop(columns=['Unnamed: 3'])

# Definir características (X) y etiquetas (y)
X = dataset.iloc[:, :2]  # Las dos primeras columnas son las características
y = dataset.iloc[:, 2]   # La tercera columna es la etiqueta

print(X)
# Dividir los datos en conjunto de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Crear el clasificador de Árbol de Decisión
clf = DecisionTreeClassifier()

# Entrenar el modelo
clf.fit(X_train, y_train)


y_predict = clf.predict(X_test)

print(X_test, y_predict)
# Exportar el árbol de decisión en formato DOT para su visualización
dot_data = export_graphviz(clf, out_file=None, 
                           feature_names=['Feature 1', 'Feature 2'],  
                           class_names=['Clase 0', 'Clase 1'],  
                           filled=True, rounded=True,  
                           special_characters=True)  

# Crear el gráfico con graphviz
graph = graphviz.Source(dot_data)

# Mostrar el gráfico
graph.view()

\end{minted}


\subsection*{Arquitecturas de Redes Neuronales}
\label{sec:org29c024e}

\subsubsection*{1. Redes Neuronales Feedforward (FFNN)}
\label{sec:org671421f}
\begin{itemize}
\item Las \textbf{\textbf{redes neuronales feedforward}} son las más simples y
consisten en capas donde la información fluye en una sola
dirección, desde la capa de entrada hasta la capa de salida, sin
retroalimentación.
\item La arquitectura de un perceptrón multicapa (MLP) es un ejemplo de
red feedforward.
\end{itemize}

\textbf{*} Aplicaciones comunes:
\begin{itemize}
\item Clasificación de datos estructurados (tabulares).
\item Problemas de regresión.
\end{itemize}

\subsubsection*{2. Redes Neuronales Convolucionales (CNN)}
\label{sec:org0fecd6d}
\begin{itemize}
\item Las \textbf{\textbf{redes neuronales convolucionales}} están diseñadas para
procesar datos que tienen una estructura de cuadrícula, como
imágenes. Las CNN usan convoluciones para reducir la cantidad de
parámetros y hacen que las redes sean eficientes para el
procesamiento de imágenes.
\end{itemize}

\textbf{*} Componentes principales:
\begin{itemize}
\item \textbf{\textbf{Capas de Convolución}}: Detectan características en los
datos, como bordes y texturas.
\item \textbf{\textbf{Capas de Pooling}}: Reducen la dimensionalidad y ayudan a
evitar el sobreajuste.
\item \textbf{\textbf{Capas de Conexión Completa}}: Al final de la red, para
clasificar o predecir.
\end{itemize}

\textbf{*} Aplicaciones comunes:
\begin{itemize}
\item Clasificación de imágenes.
\item Detección y segmentación de objetos.
\item Procesamiento de video.
\end{itemize}

\textbf{*} Ejemplo en Python:
\begin{minted}[]{python}
       import tensorflow as tf
       from tensorflow.keras import layers, models

       model = models.Sequential([
           layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),
           layers.MaxPooling2D((2, 2)),
           layers.Conv2D(64, (3, 3), activation='relu'),
           layers.MaxPooling2D((2, 2)),
           layers.Flatten(),
           layers.Dense(64, activation='relu'),
           layers.Dense(10, activation='softmax')
       ])
\end{minted}

\subsubsection*{3. Redes Neuronales Recurrentes (RNN)}
\label{sec:org26460b3}
\begin{itemize}
\item Las \textbf{\textbf{redes neuronales recurrentes}} están diseñadas para trabajar
con datos secuenciales o series temporales. Las RNN tienen una
conexión recurrente que les permite recordar información de
entradas anteriores.
\end{itemize}

\textbf{*} Variantes de RNN:
\begin{itemize}
\item \textbf{\textbf{LSTM (Long Short-Term Memory)}}: Diseñadas para retener
información en secuencias largas y resolver el problema del
gradiente desvaneciente.
\item \textbf{\textbf{GRU (Gated Recurrent Unit)}}: Variante simplificada de LSTM que es más eficiente computacionalmente.
\end{itemize}

\textbf{*} Aplicaciones comunes:
\begin{itemize}
\item Procesamiento de lenguaje natural (NLP).
\item Predicción de series temporales.
\item Generación de texto.
\end{itemize}

\textbf{*} Ejemplo en Python:
\begin{minted}[]{python}
       from tensorflow.keras.layers import SimpleRNN, LSTM, Dense
       from tensorflow.keras.models import Sequential

       model = Sequential([
           LSTM(50, input_shape=(100, 1)),
           Dense(1)
       ])
\end{minted}

\subsubsection*{4. Redes Generativas Antagónicas (GAN)}
\label{sec:org3564930}
\begin{itemize}
\item Las \textbf{\textbf{redes generativas antagónicas}} (GAN) consisten en dos redes:
una red generadora y una red discriminadora que compiten entre
sí. La red generadora intenta crear datos similares a los datos
reales, mientras que la red discriminadora trata de distinguir
entre datos reales y generados.
\end{itemize}

\textbf{*} Componentes principales:
\begin{itemize}
\item \textbf{\textbf{Generador}}: Produce datos falsos similares a los datos de entrenamiento.
\item \textbf{\textbf{Discriminador}}: Evalúa la autenticidad de los datos.
\item \textbf{\textbf{Entrenamiento}}: Ambos modelos se entrenan de forma simultánea en un proceso antagónico.
\end{itemize}

\textbf{*} Aplicaciones comunes:
\begin{itemize}
\item Generación de imágenes realistas.
\item Aumento de datos (data augmentation).
\item Transferencia de estilo.
\end{itemize}

\textbf{*} Ejemplo en Python:
\begin{minted}[]{python}
       from tensorflow.keras.layers import Dense, LeakyReLU
       from tensorflow.keras.models import Sequential

       # Modelo Generador
       generator = Sequential([
           Dense(256, input_dim=100),
           LeakyReLU(0.2),
           Dense(512),
           LeakyReLU(0.2),
           Dense(784, activation='tanh')
       ])

       # Modelo Discriminador
       discriminator = Sequential([
           Dense(512, input_shape=(784,)),
           LeakyReLU(0.2),
           Dense(256),
           LeakyReLU(0.2),
           Dense(1, activation='sigmoid')
       ])
\end{minted}

\subsubsection*{5. Redes de Memoria a Largo y Corto Plazo (LSTM)}
\label{sec:org1f8bd97}
\begin{itemize}
\item Las \textbf{\textbf{LSTM}} son una variante de las RNN que utilizan un sistema de puertas (input gate, forget gate y output gate) para controlar el flujo de información y decidir qué recordar y qué olvidar.
\end{itemize}

\textbf{*} Aplicaciones comunes:
\begin{itemize}
\item Traducción automática.
\item Generación de texto.
\item Análisis de sentimientos.
\end{itemize}

\subsubsection*{6. Redes de Transformadores}
\label{sec:org4e124e9}
\begin{itemize}
\item Los \textbf{\textbf{transformadores}} utilizan un mecanismo de auto-atención para
procesar datos secuenciales en paralelo. Esta arquitectura ha
revolucionado el procesamiento del lenguaje natural.
\end{itemize}

\textbf{*} Componentes principales:
\begin{itemize}
\item \textbf{\textbf{Auto-atención}}: Permite a la red enfocarse en diferentes
partes de la entrada sin procesarlas en orden secuencial.
\item \textbf{\textbf{Embeddings}}: Representaciones vectoriales de las palabras.
\item \textbf{\textbf{Capas de Feedforward y Atención Multi-Cabeza}}: Proveen
capacidad de aprendizaje al procesar la información.
\end{itemize}

\textbf{*} Aplicaciones comunes:
\begin{itemize}
\item Traducción automática.
\item Resumen de texto.
\item Generación de lenguaje (GPT-3, BERT).
\end{itemize}

\textbf{*} Ejemplo en Python:
\begin{minted}[]{python}
       from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

       tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
       model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")

       # Tokenización de entrada
       inputs = tokenizer("Hello, how are you?", return_tensors="tf")

       # Predicción
       outputs = model(inputs)
\end{minted}

\subsubsection*{7. Redes Autoencoders}
\label{sec:orge17f5a5}
\begin{itemize}
\item Los \textbf{\textbf{autoencoders}} son redes neuronales diseñadas para aprender representaciones de datos de alta dimensionalidad en un espacio de menor dimensión. Consisten en dos partes principales: el codificador y el decodificador.
\end{itemize}

\textbf{*} Componentes principales:
\begin{itemize}
\item \textbf{\textbf{Codificador (Encoder)}}: Reduce la dimensión del dato de entrada.
\item \textbf{\textbf{Decodificador (Decoder)}}: Reconstruye el dato original a partir de la representación reducida.
\end{itemize}

\textbf{*} Aplicaciones comunes:
\begin{itemize}
\item Compresión de imágenes.
\item Eliminación de ruido en datos.
\item Detección de fraudes.
\end{itemize}

\textbf{*} Ejemplo en Python:
\begin{minted}[]{python}
       from tensorflow.keras.layers import Input, Dense
       from tensorflow.keras.models import Model

       # Autoencoder simple
       input_img = Input(shape=(784,))
       encoded = Dense(32, activation='relu')(input_img)
       decoded = Dense(784, activation='sigmoid')(encoded)

       autoencoder = Model(input_img, decoded)
\end{minted}

\subsubsection*{8. Redes de Cápsulas (Capsule Networks)}
\label{sec:orgac0990f}
\begin{itemize}
\item Las \textbf{\textbf{Capsule Networks}} fueron propuestas para superar las limitaciones de las CNN en la detección de la rotación y posición espacial de los objetos en imágenes. Estas redes usan cápsulas (grupos de neuronas) para capturar mejor la orientación y jerarquía en las características de la imagen.
\end{itemize}

\textbf{*} Aplicaciones comunes:
\begin{itemize}
\item Clasificación de imágenes donde la orientación y posición de los objetos son relevantes.
\end{itemize}


\section*{Análisis de información}
\label{sec:org494c0a1}
\subsection*{Programación}
\label{sec:org45567aa}
\subsubsection*{Carga de imágenes y propiedades}
\label{sec:orgd324d7c}

Con el siguiente ejemplo, se puede cargar una imagen, utilizando la
librería de opencv, mediante el método \href{https://docs.opencv.org/3.4/d4/da8/group\_\_imgcodecs.html}{imread}, el cual carga en la
variable \textbf{\textbf{img}} la imagen contenida en el directorio que se puede
observar en la linea 2 de igual forma el método \href{https://www.geeksforgeeks.org/python-opencv-cv2-imshow-method/}{imshow} permite mostrar
la imagen, dando como primer parámetro, el nombre del marco y
posteriormente el nombre de la imagen, finalmente se utiliza el método
\href{https://www.geeksforgeeks.org/python-opencv-waitkey-function/}{waitKey} permite  mostrar una ventana durante un número
específico de milisegundos o hasta que se presione cualquier tecla, a su vez 
la el método \href{https://www.geeksforgeeks.org/python-opencv-destroyallwindows-function/}{destroyAllWindows} permite destruir todas las ventanas abiertas 
creadas por \textbf{\textbf{imshow}}. 



\begin{minted}[]{python}
import cv2 as cv 
img = cv.imread("/home/likcos/Imágenes/tr.png")
cv.imshow('marco', img)
cv.waitKey()
cv.destroyAllWindows()
\end{minted}

Opencv también permite acceder a las propiedades de la imagen mediante
la función \href{https://docs.opencv.org/3.4/d3/df2/tutorial\_py\_basic\_ops.html}{shape}, con la cual podremos acceder al tamaño de la imagen,
los canales de color entre otras propiedades. En el ejemplo siguiente
se muestra como al cargar la imagen utilizando la bandera de \textbf{\textbf{1}}
podemos acceder al ancho, alto y al numero de canales de color de la
imagen. En caso de cargar la imagen utilizando el \textbf{\textbf{0}}, \textbf{\textbf{shape}}
solo podrá acceder al alto, ancho de la imagen.


\begin{minted}[]{python}
import cv2 as cv 
img = cv.imread("/home/likcos/Imágenes/tr.png",1)
w,h,c = img.shape
print(w,h,c)
cv.imshow('marco', img)
cv.waitKey()
cv.destroyAllWindows()

\end{minted}
\subsubsection*{Operadores Puntuales}
\label{sec:org12fb495}

De igual forma cargando la imagen, en un solo canal es posible aplicar
operadores puntuales a la imagen. Las operaciones puntuales son
transformaciones de uno a uno, es decir el nuevo valor de un pixel 'q'
en la posición ( i , j ) esta en función de un pixel 'p' de otra
imagen pero en la misma posición, es decir, ( i , j ).

\begin{minted}[]{python}
import cv2 as cv
import numpy as np

img = cv.imread('tr.png', 0)
print(img.shape, img.size)
w = img.shape[0]
h = img.shape[1]
#c = img.shape[2]
#w1, h1 = img.shape
img2 = img
cv.imshow('imagen', img2)
for x in range(w):
    for y in range(h):
        if(img[x,y]>50):
            img[x,y]=255
        else:
            img[x,y]=0

cv.imshow('imagen1', img)
cv.waitKey(0)
cv.destroyAllWindows()
\end{minted}

\subsubsection*{Modelos de color}
\label{sec:org8210345}

El color es una de las características que nos permite a los seres
humanos identificar y clasificar los objetos. La percepción visual del
color se genera en nuestro cerebro a partir de las ondas
electromagnéticas reflejadas por los objetos y captadas por los ojos.
Desde el punto de vista del procesamiento de imágenes por computador,
es preciso recurrir a los llamados espacios de color, se trata de
conjuntos de fórmulas matemáticas que permiten describir los colores y
descomponerlos en distintos canales. Los espacios de color más
utilizados son el RGB y el CMYK, debido a que el modelo RGB se utiliza
en periféricos como pantallas, cámaras y escáneres, y el modelo CMYK
en impresoras. En este capítulo se analizará también el sistema de
coordenadas tridimensional (tono, saturación e intensidad) del espacio
de color HSI, donde cada color está representado por un punto en el
espacio. Los dos espacios siguientes que se estudiarán fueron
establecidos por la Comisión Internacional de Iluminación (CIE): el
espacio de color XYZ se usa actualmente como referencia para definir
los colores que percibe el ojo humano, mientras que el Lab se puede
considerar como el más completo de los desarrollados por la CIE, ya
que permite identificar cada color de una forma muy precisa mediante
sus valores “a” y “b” y su brillo (“L”).  Finalmente, se analizará el
espacio de color YCbCr.

\begin{minted}[]{python}
img = cv.imread('tr.png', 1)
img2 = cv.cvtColor(img, cv.COLOR_BGR2RGB)
cv.imshow('img', img)
cv.imshow('img2', img2)

zero = np.zeros(img.shape[:2], dtype='uint8')
(r,g,b)= cv.split(img)
#cv.imshow('rg', r)
#cv.imshow('gg', g)
#cv.imshow('bg', b)
cv.imshow('GRB', cv.merge([g,r,b]))
#cv.imshow('G', cv.merge([zero,g,zero]))
#cv.imshow('R', cv.merge([zero,zero,r]))
#(r1,g1,b1)= cv.split(img2)
#cv.imshow('R1', cv.merge([r1,zero,zero]))
#cv.imshow('G1', cv.merge([zero,g1,zero]))
#cv.imshow('B1', cv.merge([zero,zero,b1]))

cv.waitKey(0)
cv.destroyAllWindows()
\end{minted}

\subsubsection*{Segmentación de Color}
\label{sec:orgd63b589}

\begin{minted}[]{python}
import cv2 as cv
img = cv.imread('salida.jpg', 1)
img2 = cv.cvtColor(img, cv.COLOR_BGR2RGB)
img3 = cv.cvtColor(img2, cv.COLOR_RGB2HSV)

umbralBajo=(0, 80, 80  )
umbralAlto=(10, 255, 255)
umbralBajoB=(170, 80,80)
umbralAltoB=(180, 255, 255)


mascara1 = cv.inRange(img3, umbralBajo, umbralAlto)
mascara2 = cv.inRange(img3, umbralBajoB, umbralAltoB)

mascara = mascara1 + mascara2

resultado = cv.bitwise_and(img, img, mask=mascara)

cv.imshow('resultado', resultado)
cv.imshow('mascara', mascara)
cv.imshow('img',img)
cv.imshow('img2', img2)
cv.imshow('img3', img3)

cv.waitKey(0)
cv.destroyAllWindows()
\end{minted}


\subsubsection*{Haarcascades}
\label{sec:org0d1b91a}
Los Haar Cascades son una técnica utilizada en el campo de la visión
por computadora para la detección de objetos. Fueron introducidos por
Paul Viola y Michael Jones en su artículo seminal "Rapid Object
Detection using a Boosted Cascade of Simple Features" en 2001. Esta
técnica es particularmente conocida por su eficacia en la detección de
rostros, aunque puede ser utilizada para detectar otros tipos de
objetos.

\begin{center}
\includegraphics[width=0.3\textwidth]{img/cascade.png}
\end{center}

\begin{itemize}
\item Conceptos Clave:
\label{sec:orgc24410c}
Características de Haar: Son patrones visuales simples que se pueden
 calcular rápidamente en una imagen. Estas características se asemejan
 a pequeñas versiones de núcleos de wavelet de Haar y son utilizadas
 para capturar la presencia de bordes, cambios de textura, y otras
 propiedades visuales.


\item Imágenes Integrales:
\label{sec:org0cd040c}
Para acelerar el cálculo de las características
 de Haar, se utiliza un concepto llamado imagen integral. Una imagen
 integral permite calcular la suma de los valores de los píxeles en
 cualquier área rectangular de la imagen en tiempo constante.

\item Adaboost:
\label{sec:org433c2a3}
Es un método de aprendizaje automático utilizado para
 mejorar la eficiencia de la detección. Selecciona un pequeño número
 de características críticas de un conjunto más grande y construye
 clasificadores "débiles". Luego, estos se combinan en un clasificador
 más fuerte y eficiente.

\item Cascadas:
\label{sec:org15b2fee}
En lugar de aplicar todas las características a una ventana de la
imagen, se organizan en una secuencia de etapas (cascadas). Cada etapa
tiene su propio clasificador (hecho con Adaboost) y solo pasa las
ventanas de la imagen que parecen prometedoras. Esto reduce
significativamente el tiempo de cálculo, ya que muchas ventanas no
pasan las primeras etapas.

 \textbf{Proceso de Detección}: 
Pre-procesamiento: Se convierte la imagen en
 escala de grises y se crea su imagen integral.

\textbf{Aplicación de las Características}: Se desplaza una ventana sobre la
imagen, y en cada posición, se calculan las características de Haar.

\textbf{Clasificación en Cascada}: Cada ventana es evaluada a través de la
cascada de clasificadores. Si una ventana falla en alguna etapa, se
descarta. Si pasa todas las etapas, se considera como una detección.

\textbf{Post-procesamiento}: Finalmente, se pueden aplicar técnicas como la
supresión de no máximos para reducir falsos positivos y mejorar la
precisión.

\textbf{Aplicaciones}: Detección de rostros en imágenes y videos.  Detección
de peatones u otros objetos en sistemas de vigilancia.  Aplicaciones
de realidad aumentada.  Es importante mencionar que, aunque los Haar
Cascades fueron revolucionarios en su momento, han sido superados en
precisión y velocidad por técnicas más modernas de aprendizaje
profundo. Sin embargo, siguen siendo utilizados debido a su
simplicidad y bajo requerimiento de recursos computacionales.

\item Ejemplo de un Haarcascade
\label{sec:org2480b5d}

\url{https://github.com/opencv/opencv/tree/master/data/haarcascades}

\url{https://opencv-python-tutroals.readthedocs.io/en/latest/py\_tutorials/py\_objdetect/py\_face\_detection/py\_face\_detection.html}

\url{https://docs.opencv.org/2.4/doc/user\_guide/ug\_traincascade.html}

\url{https://amin-ahmadi.com/cascade-trainer-gui/}
\begin{minted}[]{python}
import numpy as np
import cv2 as cv
import math 

rostro = cv.CascadeClassifier('haarcascade_frontalface_alt.xml')
cap = cv.VideoCapture(0)
i = 0  
while True:
    ret, frame = cap.read()
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    rostros = rostro.detectMultiScale(gray, 1.3, 5)
    for(x, y, w, h) in rostros:
       #frame = cv.rectangle(frame, (x,y), (x+w, y+h), (0, 255, 0), 2)
       frame2 = frame[ y:y+h, x:x+w]
       #frame3 = frame[x+30:x+w-30, y+30:y+h-30]
       frame2 = cv.resize(frame2, (100, 100), interpolation=cv.INTER_AREA)
       cv.imwrite('/home/likcos/recorte/lalo'+str(i)+'.jpg', frame2)
       cv.imshow('rostror', frame2)
    cv.imshow('rostros', frame)
    i = i+1
    k = cv.waitKey(1)
    if k == 27:
        break
cap.release()
cv.destroyAllWindows()
\end{minted}

\begin{minted}[]{python}
import cv2 as cv 

rostro = cv.CascadeClassifier('haarcascade_frontalface_alt.xml')
cap = cv.VideoCapture(0)

while True:
    ret, img = cap.read()
    gris = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
    rostros = rostro.detectMultiScale(gris, 1.3, 5)
    for(x,y,w,h) in rostros:
        res = int((w+h)/8)
        img = cv.rectangle(img, (x,y), (x+w, y+h), (234, 23,23), 2)
        img = cv.rectangle(img, (x,int(y+h/2)), (x+w, y+h), (0,255,0),5 )
        img = cv.circle(img, (x + int(w*0.3), y + int(h*0.4)) , 21, (0, 0, 0), 2 )
        img = cv.circle(img, (x + int(w*0.7), y + int(h*0.4)) , 21, (0, 0, 0), 2 )
        img = cv.circle(img, (x + int(w*0.3), y + int(h*0.4)) , 20, (255, 255, 255), -1 )
        img = cv.circle(img, (x + int(w*0.7), y + int(h*0.4)) , 20, (255, 255, 255), -1 )
        img = cv.circle(img, (x + int(w*0.3), y + int(h*0.4)) , 5, (0, 0, 255), -1 )
        img = cv.circle(img, (x + int(w*0.7), y + int(h*0.4)) , 5, (0, 0, 255), -1 )

    cv.imshow('img', img)
    if cv.waitKey(1)== ord('q'):
        break
    
cap.release
cv.destroyAllWindows()
\end{minted}
\end{itemize}





\subsubsection*{Template Matching}
\label{sec:orgf623c04}

El \textbf{\textbf{Template Matching}} (emparejamiento de plantillas) es una técnica utilizada en procesamiento de imágenes y visión por computadora para buscar y encontrar una sub-imagen o patrón dentro de una imagen más grande. 

\textbf{\textbf{Descripción del algoritmo Template Matching}}

\begin{enumerate}
\item \textbf{\textbf{Entrada}}:
\begin{itemize}
\item Una \textbf{\textbf{imagen de entrada}}: Es la imagen en la cual se desea buscar la plantilla.
\item Una \textbf{\textbf{plantilla}}: Es una sub-imagen más pequeña que se quiere localizar dentro de la imagen de entrada.
\end{itemize}

\item \textbf{\textbf{Desplazamiento de la plantilla}}: 
\begin{itemize}
\item El algoritmo desplaza la plantilla sobre la imagen de entrada y compara las intensidades de los píxeles de la plantilla con los de la región correspondiente en la imagen.
\end{itemize}

\item \textbf{\textbf{Cálculo de la similitud}}:
\begin{itemize}
\item Existen varias formas de medir la similitud entre la plantilla y la región correspondiente en la imagen:
\begin{itemize}
\item \textbf{\textbf{Correlación Normalizada (NCC)}}: La correlación normalizada mide el grado de similitud entre la plantilla y la región de la imagen.
\item \textbf{\textbf{Suma de diferencias cuadráticas (SSD)}}: Esta métrica mide la diferencia entre los valores de los píxeles correspondientes en la plantilla y la imagen. Se busca minimizar esta diferencia.
\item \textbf{\textbf{Suma de diferencias absolutas (SAD)}}: Similar a la SSD, pero se suma la diferencia absoluta entre los valores de los píxeles.
\end{itemize}
\end{itemize}

\item \textbf{\textbf{Localización de la mejor coincidencia}}:
\begin{itemize}
\item El algoritmo calcula la similitud o diferencia para cada posible ubicación de la plantilla en la imagen de entrada. La posición con el valor más alto de similitud (o más bajo de diferencia) será la ubicación donde la plantilla se ajusta mejor a la imagen.
\end{itemize}

\item \textbf{\textbf{Resultado}}:
\begin{itemize}
\item El algoritmo devuelve las coordenadas de la región que coincide mejor con la plantilla.
\end{itemize}
\end{enumerate}

\textbf{\textbf{Formulaciones comunes}}

\begin{enumerate}
\item \textbf{\textbf{Correlación Normalizada (NCC)}}:
\end{enumerate}
\(R(x, y) = \frac{\sum_{u,v} \left[ T(u, v) - \bar{T} \right] \left[ I(x+u, y+v) - \bar{I} \right]}{\sqrt{\sum_{u,v} \left[ T(u, v) - \bar{T} \right]^2 \sum_{u,v} \left[ I(x+u, y+v) - \bar{I} \right]^2}}\)

\begin{enumerate}
\item \textbf{\textbf{Suma de diferencias cuadráticas (SSD)}}:
\end{enumerate}
\(R(x, y) = \sum_{u,v} \left[ I(x+u, y+v) - T(u, v) \right]^2\)

\begin{enumerate}
\item \textbf{\textbf{Suma de diferencias absolutas (SAD)}}:
\end{enumerate}
\(R(x, y) = \sum_{u,v} \left| I(x+u, y+v) - T(u, v) \right|\)

\textbf{\textbf{Ventajas del Template Matching}}

\begin{itemize}
\item \textbf{\textbf{Simplicidad}}: Es un método sencillo y directo de encontrar una sub-imagen dentro de otra.
\item \textbf{\textbf{Determinístico}}: No requiere entrenamiento ni ajustes de parámetros complejos.
\item \textbf{\textbf{Aplicaciones}}: Útil en aplicaciones de localización precisa.
\end{itemize}

\textbf{\textbf{Desventajas del Template Matching}}

\begin{itemize}
\item \textbf{\textbf{Sensibilidad a la escala y rotación}}: No funciona bien si la plantilla y la imagen no están en la misma escala o rotadas.
\item \textbf{\textbf{Ruido y variaciones de iluminación}}: Se ve afectado por el ruido y las variaciones de iluminación.
\item \textbf{\textbf{Costoso computacionalmente}}: Es lento si la imagen es grande, ya que requiere hacer muchas comparaciones.
\end{itemize}

\textbf{\textbf{Mejoras del Template Matching}}

\begin{itemize}
\item \textbf{\textbf{Uso de pirámides}}: Se pueden utilizar pirámides de imágenes para mejorar el rendimiento.
\item \textbf{\textbf{Métodos más robustos}}: Se pueden utilizar métodos avanzados de coincidencia basados en características locales, como SIFT o SURF.
\end{itemize}


\begin{minted}[]{python}
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Cargar la imagen y la plantilla en color
imagen_color = cv2.imread('gen.png')  # Imagen donde se buscará la plantilla
plantilla_color = cv2.imread('wall.png')   # La plantilla que queremos encontrar

# Convertir las imágenes a escala de grises para el template matching
imagen_gris = cv2.cvtColor(imagen_color, cv2.COLOR_BGR2GRAY)
plantilla_gris = cv2.cvtColor(plantilla_color, cv2.COLOR_BGR2GRAY)

# Obtener las dimensiones de la plantilla
altura, ancho = plantilla_gris.shape

# Aplicar el método de Template Matching usando la correlación normalizada (TM_CCOEFF_NORMED)
resultado = cv2.matchTemplate(imagen_gris, plantilla_gris, cv2.TM_CCOEFF_NORMED)

# Encontrar la mejor coincidencia usando minMaxLoc
min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(resultado)

# Coordenadas del rectángulo de coincidencia
top_left = max_loc
bottom_right = (top_left[0] + ancho, top_left[1] + altura)

# Dibujar un rectángulo alrededor de la coincidencia sobre la imagen original en color
cv2.rectangle(imagen_color, top_left, bottom_right, (0, 255, 0), 2)

# Convertir BGR a RGB para mostrar con matplotlib
#imagen_rgb = cv2.cvtColor(imagen_color, cv2.COLOR_BGR2RGB)

# Mostrar la imagen con el rectángulo en color
cv2.imshow('resultado', imagen_color)
cv2.imshow('resultado2', resultado)

cv2.waitKey()
cv2.destroyAllWindows()

\end{minted}



\section*{Programación}
\label{sec:org4f45453}

\begin{minted}[]{python}
import cv2 as cv
import numpy as np 
img = cv.imread('tr.png',0)
x,y=img.shape
img2 = np.zeros((x*2,y*2), dtype='uint8')
for i in range(x):
    for j in range(y):
        if(img[i,j]>150):
            img[i, j]=255
        else:
            img[i,j] = 0

print(img.shape)
#cv.imshow('img2', img2)
cv.imshow('img', img)
cv.waitKey(0)
cv.destroyAllWindows()

\end{minted}

\begin{minted}[]{python}
import cv2 as cv

cap = cv.VideoCapture(0)

while(True):
    ret, img =cap.read()
    if ret == True:
        cv.imshow('img', img)
    	k =cv.waitKey(1) & 0xFF
	if k == 27 :
	    break
    else:
	break

cap.release()
cv.destroyAllWindows()

\end{minted}

\begin{minted}[]{python}
import cv2 as cv
import numpy as np 
img = cv.imread('tr.png',0)
print(img.shape)
x,y = img.shape
img2 = np.zeros((x*2,y*2), dtype='uint8')
for i in range(x):
    for j in range(y):
        img2[i*2,j*2]=img[i,j]

cv.imshow('img', img)
cv.imshow('img2', img2)
cv.waitKey(0)
cv.destroyAllWindows()

\end{minted}


\begin{minted}[]{python}
import cv2 as cv 

img = cv.imread('tr.png', 1)
gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
rgb = cv.cvtColor(img, cv.COLOR_BGR2RGB)
hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)

cv.imshow('img', img)
cv.imshow('gray', gray)
cv.imshow('rgb', rgb)
cv.imshow('hsv', hsv)

cv.waitKey(0)
cv.destroyAllWindows()

\end{minted}

\begin{minted}[]{python}
import cv2 as cv
import numpy as np 

cap = cv.VideoCapture(0)

while(True):
    ret,img=cap.read()
    if(ret == True):
        cv.imshow('marco', img)
        x,y = img.shape[:2]
        img2 = np.zeros((x,y), dtype='uint8')
       
        #hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)
        #cv.imshow('hsv', hsv)
        b,g,r = cv.split(img)
        bm = cv.merge([b,img2, img2])
        gm = cv.merge([img2, g, img2])
        rm = cv.merge([img2, img2, r])
        eje = cv.merge([b,r,g])
        cv.imshow('b', bm)
        cv.imshow('g', gm)
        cv.imshow('r', rm)
        cv.imshow('eje', eje)
        k =cv.waitKey(1) & 0xFF
	if k == 27 :
	    break
    else:
        break

cap.release()
cv.destroyAllWindows()
\end{minted}

\begin{minted}[]{python}
import cv2 as cv

img = cv.imread('man1.jpg', 1)
hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)
ubb=(0, 100, 100)
uba=(10, 255, 255)
ubb2=(170, 100, 100)
uba2=(180, 255, 255)

mask1 = cv.inRange(hsv, ubb, uba)
mask2 = cv.inRange(hsv, ubb2, uba2)
mask = mask1 + mask2


res = cv.bitwise_and(img, img, mask=mask)
cv.imshow('mask', mask)
cv.imshow('hsv', hsv)
cv.imshow('res', res)
cv.imshow('img', img)
cv.waitKey(0)
cv.destroyAllWindows()




\end{minted}

\subsection*{Segmentación de color en vídeo}
\label{sec:org3cea86d}

\begin{minted}[]{python}
import cv2 as cv
import numpy as np 

cap = cv.VideoCapture(0)

while(True):
    ret,img=cap.read()
    if(ret == True):
        hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)
        ubb=(35, 40, 40)
        uba=(95, 255, 255)

        mask = cv.inRange(hsv, ubb, uba)
        res = cv.bitwise_or(img, img, mask=mask)

        cv.imshow('img', img)
        cv.imshow('res', res)
        
        k =cv.waitKey(1) & 0xFF
	if k == 27 :
	    break
    else:
        break

cap.release()
cv.destroyAllWindows()
\end{minted}

\subsection*{Seguimiento Por color}
\label{sec:orge009042}

\begin{minted}[]{python}
import cv2
import numpy as np

# Iniciar la captura de video desde la cámara
cap = cv2.VideoCapture(0)
# Definir el rango de color que quieres rastrear en el espacio de color HSV (en este caso, azul)
lower_blue = np.array([100, 150, 0])
upper_blue = np.array([140, 255, 255])
img2=None
i=0 
while True:
    # Capturar frame por frame
    ret, frame = cap.read()
    if not ret:
        break
    
    # Convertir el frame de BGR a HSV
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    
    # Crear una máscara que detecte solo el color azul
    mask = cv2.inRange(hsv, lower_blue, upper_blue)
    
    # Filtrar la máscara con operaciones morfológicas
    mask = cv2.erode(mask, None, iterations=2)
    mask = cv2.dilate(mask, None, iterations=2)
    
    # Encontrar contornos en la máscara
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    # Si se encuentra al menos un contorno, seguir el objeto
    if contours:
        # Tomar el contorno más grande
        largest_contour = max(contours, key=cv2.contourArea)
        
        # Encontrar el centro del contorno usando un círculo mínimo que lo rodee
        ((x, y), radius) = cv2.minEnclosingCircle(largest_contour)
        
        # Dibujar el círculo y el centro en el frame original si el radio es mayor que un umbral
        if radius > 10:
            i=i+1
            #cv2.circle(frame, (int(x), int(y)), int(radius), (0, 255, 255), 2)
            #cv2.circle(frame, (int(x), int(y)), 5, (0, 255, 255), -1)
            #cv2.rectangle(frame, (int(x-radius), int(y-radius)), (int(x+radius), int(y+radius)), (0, 0, 255), 3)
            img2 = frame[int(y-radius):int(y+radius), int(x-radius):int(x+radius)]
            cv2.imwrite('/home/likcos/recorte/recorte'+str(i)+'.jpg',  img2)
            
            cv2.imshow('img2', img2)
    # Mostrar el frame
    cv2.imshow('Frame', frame)
    #cv2.imshow('img2', img2)
    #cv2.imshow('Mask', mask)

    # Salir si se presiona la tecla 'q'
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Liberar la captura y cerrar todas las ventanas
cap.release()
cv2.destroyAllWindows()

\end{minted}




\section*{Tutorial Básico de Pygame}
\label{sec:org19bf42b}

Pygame es una biblioteca de Python que facilita la creación de
videojuegos y aplicaciones gráficas interactivas. Este tutorial te
guiará a través de los conceptos básicos de Pygame, desde la
instalación hasta la creación de una ventana básica y la interacción
con el teclado.

\begin{itemize}
\item \href{https://www.pygame.org/docs/}{Documentación Oficial de Pygame}
\end{itemize}


\subsection*{Instalación de Pygame}
\label{sec:orgda764a3}
Para instalar Pygame, puedes usar `pip`, el gestor de paquetes de Python. Abre tu terminal y ejecuta el siguiente comando:


\begin{minted}[]{sh}
pip install pygame
\end{minted}

\subsection*{Creación de una ventana básica}
\label{sec:org8b3a8b0}

El primer paso para cualquier aplicación en Pygame es crear una ventana. Este código inicializa Pygame y abre una ventana de 800x600 píxeles.

\begin{minted}[]{python}
import pygame
import sys

# Inicialización de Pygame
pygame.init()

# Tamaño de la ventana
ANCHO, ALTO = 800, 600
ventana = pygame.display.set_mode((ANCHO, ALTO))
pygame.display.set_caption("Ventana Básica en Pygame")

# Ciclo principal
while True:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            pygame.quit()
            sys.exit()

    # Rellenar la ventana con un color (blanco)
    ventana.fill((255, 255, 255))

    # Actualizar la ventana
    pygame.display.flip()
\end{minted}

\subsection*{Manejo de eventos}
\label{sec:orgf946871}

Los eventos en Pygame incluyen entradas del teclado, del ratón, y el cierre de la ventana. A continuación te muestro cómo capturar eventos de teclado.

\begin{minted}[]{python}
import pygame
import sys

# Inicialización de Pygame
pygame.init()

# Tamaño de la ventana
ANCHO, ALTO = 800, 600
ventana = pygame.display.set_mode((ANCHO, ALTO))
pygame.display.set_caption("Manejo de Eventos en Pygame")

# Colores
BLANCO = (255, 255, 255)
ROJO = (255, 0, 0)

# Posición inicial del cuadrado
x, y = 100, 100
velocidad = 5

# Ciclo principal
while True:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            pygame.quit()
            sys.exit()

    # Movimiento del cuadrado con teclas de flechas
    keys = pygame.key.get_pressed()
    if keys[pygame.K_LEFT]:
        x -= velocidad
    if keys[pygame.K_RIGHT]:
        x += velocidad
    if keys[pygame.K_UP]:
        y -= velocidad
    if keys[pygame.K_DOWN]:
        y += velocidad

    # Dibujar el cuadrado
    ventana.fill(BLANCO)
    pygame.draw.rect(ventana, ROJO, (x, y, 50, 50))

    # Actualizar la pantalla
    pygame.display.flip()
\end{minted}

\subsection*{Dibujar formas y texto}
\label{sec:org3f285a9}

Pygame facilita el dibujo de formas básicas, como rectángulos, círculos, y también permite renderizar texto.

\begin{minted}[]{python}
import pygame
import sys

# Inicialización de Pygame
pygame.init()

# Tamaño de la ventana
ANCHO, ALTO = 800, 600
ventana = pygame.display.set_mode((ANCHO, ALTO))
pygame.display.set_caption("Dibujo de Formas y Texto en Pygame")

# Colores
BLANCO = (255, 255, 255)
VERDE = (0, 255, 0)
AZUL = (0, 0, 255)

# Fuente de texto
fuente = pygame.font.SysFont(None, 55)

# Ciclo principal
while True:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            pygame.quit()
            sys.exit()

    # Rellenar la ventana
    ventana.fill(BLANCO)

    # Dibujar un rectángulo
    pygame.draw.rect(ventana, VERDE, (150, 150, 200, 100))

    # Dibujar un círculo
    pygame.draw.circle(ventana, AZUL, (400, 300), 75)

    # Renderizar texto
    texto = fuente.render("Hola, Pygame!", True, AZUL)
    ventana.blit(texto, (250, 500))

    # Actualizar la ventana
    pygame.display.flip()
\end{minted}

\subsection*{Cargar y mostrar imágenes}
\label{sec:org3435e87}

Pygame permite cargar imágenes desde archivos externos. Aquí te muestro cómo hacerlo:

\begin{minted}[]{python}
import pygame
import sys

# Inicialización de Pygame
pygame.init()

# Tamaño de la ventana
ANCHO, ALTO = 800, 600
ventana = pygame.display.set_mode((ANCHO, ALTO))
pygame.display.set_caption("Mostrar Imágenes en Pygame")

# Cargar la imagen
imagen = pygame.image.load('ruta_a_tu_imagen.png')

# Ciclo principal
while True:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            pygame.quit()
            sys.exit()

    # Dibujar la imagen en la ventana
    ventana.fill((255, 255, 255))
    ventana.blit(imagen, (100, 100))

    # Actualizar la ventana
    pygame.display.flip()
\end{minted}

\section*{Web scraping}
\label{sec:org487403b}

\begin{minted}[]{python}
import snscrape.modules.twitter as sntwitter

query = "ley fula de tal"
tetes = []

for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):
    if i > 10:  # Limitar a los primeros 10 tweets
        break
    tweets.append((tweet.user.username, tweet.content))

for user, content in tweets:
    print(f"Usuario: {user}")
    print(f"Texto: {content}\n")
\end{minted}



\section*{Preguntas}
\label{sec:org907b42d}

\subsection*{Preguntas para responder con el estudio y análisis a través de algoritmos de inteligencia artificial.}
\label{sec:org8693d1d}

\subsubsection*{Preguntas Ley al poder Judicial}
\label{sec:org701fb73}
\begin{itemize}
\item ¿El Diagnostico de la ley al poder judicial es conocido y qué estudios expertos tuvieron en cuenta?
\item ¿Por qué la reforma no incluyó a las fiscalías a las defensoría y
sólo se limitó al poder judicial ?
\item ¿Qué medidas concretas se van a implementar para evitar la captación
del crimen organizado y la violencia en el contexto electoral?
\item ¿Cómo garantizar que juristas probos y honestos se animen a competir
públicamente frente a los riesgos de la violencia y que la campaña
para ser incluidos en las listas no implique negociaciones indebidas?
\item ¿Cómo se conforman estos comités de postulación?
\item ¿Cómo asegurar la carrera judicial?
\item ¿Cómo compatibilizar la incorporación de medidas para preservar la
identidad de los jueces conocidos en el sistema interamericano como
"jueces sin rostro" con los estándares interamericanos ?
\item ¿Cómo impactará el enorme costo económico que tendrá la
implementación de esta reforma con la promoción y ?
\end{itemize}

\subsubsection*{Ley Organismos Autónomos}
\label{sec:org13be7bf}
\begin{itemize}
\item ¿Es constitucional esta ley, considerando que algunos organismos
autónomos están establecidos en la Constitución?
\item ¿Cómo afectaría la eliminación de estos organismos a la
transparencia y rendición de cuentas en el gobierno?
\item ¿Qué funciones críticas podrían perder independencia y control al
pasar al poder ejecutivo o a otras instituciones?
\item ¿Existen alternativas para mejorar la eficiencia de los organismos
autónomos sin eliminarlos?
\item ¿Qué sectores de la sociedad civil y grupos de interés se verían
afectados por la desaparición de estos organismos?
\end{itemize}




\section*{Introducción a los  Embeddings}
\label{sec:orgb926d51}
Los \textbf{embeddings} son representaciones vectoriales densas y de baja
dimensión de elementos de un espacio de características, diseñadas
para capturar relaciones semánticas o estructurales entre dichos
elementos. Este concepto es ampliamente utilizado en el procesamiento
del lenguaje natural (NLP), aprendizaje automático y redes neuronales.

\subsection*{Concepto de Embedding}
\label{sec:org68e57fe}
Un \textbf{embedding} toma objetos de un espacio discreto (como palabras,
frases, imágenes, o nodos de un grafo) y los transforma en un espacio
continuo, típicamente un espacio vectorial de dimensión reducida.

Por ejemplo, en NLP, palabras como "gato" y "perro" pueden tener
embeddings que las representan como vectores cercanos en un espacio de
características, indicando su similitud semántica.

\subsubsection*{Propiedades de los Embeddings}
\label{sec:orge526b4f}
\begin{itemize}
\item Representación densa: Cada elemento está representado por un vector
con valores en un espacio continuo.
\item Capacidad semántica: Capturan similitudes y relaciones jerárquicas.
\item Reducción dimensional: Transforman datos de alta dimensionalidad en
un espacio más manejable.
\end{itemize}

\subsection*{Ejemplos de Uso}
\label{sec:orgd7361a6}
\subsubsection*{Procesamiento del Lenguaje Natural (NLP)}
\label{sec:org7f4311c}
En NLP, los embeddings como Word2Vec, GloVe o FastText convierten
palabras en vectores. Estos vectores permiten realizar tareas como:
\begin{itemize}
\item Análisis de sentimientos.
\item Traducción automática.
\item Recuperación de información.
\end{itemize}

\subsubsection*{Recomendación de Contenido}
\label{sec:org720c3c6}
Los embeddings son fundamentales para sistemas de
recomendación. Representan usuarios y elementos (como películas,
productos, etc.) en un espacio vectorial, donde la proximidad indica
relevancia.

\subsection*{Visualización y Reducción Dimensional}
\label{sec:org8b9e435}
Algoritmos como t-SNE o UMAP permiten visualizar datos de alta
dimensión reducidos a espacios de 2D o 3D mediante embeddings.

\subsection*{Métodos para Generar Embeddings}
\label{sec:orgd1184ff}
\subsubsection*{Redes Neuronales}
\label{sec:org6099e55}
Redes como autoencoders o modelos preentrenados (BERT, GPT) generan
embeddings sofisticados al capturar relaciones complejas en los datos.

\subsubsection*{Factorización Matricial}
\label{sec:org17f054c}
Técnicas como Singular Value Decomposition (SVD) y métodos
colaborativos generan embeddings basados en relaciones entre filas y
columnas.

\subsubsection*{Modelos Gráficos}
\label{sec:org5867304}
Graph Embeddings como Node2Vec o DeepWalk transforman nodos de un
grafo en vectores preservando relaciones topológicas.

\subsection*{Implementación Básica}
\label{sec:org63acfbf}
Aquí un ejemplo en Python utilizando \texttt{sklearn} para generar embeddings
básicos:

\begin{minted}[]{python}
from sklearn.manifold import TSNE
import numpy as np

# Datos de ejemplo (3D)
data = np.array([[1, 2, 3], [2, 3, 4], [5, 6, 7]])

# Generar un embedding 2D
tsne = TSNE(n_components=2)
embedding = tsne.fit_transform(data)

print("Embeddings 2D:", embedding)
\end{minted}


\section*{¿Qué es Ollama?}
\label{sec:org8fb01cb}
\textbf{\textbf{Ollama}} es una herramienta diseñada para integrar y facilitar el uso
de modelos de inteligencia artificial centrados en el procesamiento
del lenguaje natural (NLP). Su objetivo principal es proporcionar un
entorno eficiente para la generación y manejo de texto, ofreciendo
capacidades avanzadas para tareas como redacción automática, análisis
semántico y generación de respuestas contextualmente relevantes.

\subsection*{Características Principales}
\label{sec:org4dd3893}
\begin{enumerate}
\item \textbf{\textbf{Modelos Preentrenados}}: Ollama emplea modelos avanzados de
lenguaje que pueden adaptarse a diversos contextos y tareas
específicas, aprovechando arquitecturas modernas como Transformers.

\item \textbf{\textbf{Integración Sencilla}}: La herramienta permite integrar modelos en
flujos de trabajo existentes mediante APIs o interfaces compatibles
con múltiples lenguajes de programación.

\item \textbf{\textbf{Personalización}}: Los usuarios pueden ajustar los modelos para
cumplir con requisitos específicos, adaptando el comportamiento del
sistema a necesidades únicas.

\item \textbf{\textbf{Procesamiento en Tiempo Real}}: Ollama es capaz de generar
respuestas o textos en tiempo real, lo que lo hace adecuado para
aplicaciones como chatbots, asistentes virtuales y análisis
dinámico de contenido.
\end{enumerate}

\subsection*{Usos Comunes de Ollama}
\label{sec:orge036e2c}
\subsubsection*{Asistentes Virtuales}
\label{sec:orgd9945b3}
Ollama se utiliza para crear asistentes virtuales que entienden y responden preguntas humanas de forma natural y precisa.

\subsubsection*{Análisis de Texto}
\label{sec:org700da74}
Las capacidades de NLP de Ollama permiten realizar análisis como:
\begin{itemize}
\item Extracción de información clave.
\item Resúmenes automáticos.
\item Clasificación de documentos.
\end{itemize}

\subsubsection*{Automatización de Redacción}
\label{sec:org1eab366}
Es ideal para la generación de contenido en masa, como publicaciones en blogs, correos electrónicos personalizados y otros textos creativos.

\subsection*{Ejemplo de Uso}
\label{sec:orgb72a176}
Un caso práctico de integración de Ollama podría ser mediante una API para generar respuestas automáticas en un chatbot:

\begin{minted}[]{python}
import ollama

# Configuración de cliente
client = ollama.Client(api_key="TU_API_KEY")

# Enviar consulta
response = client.query("¿Cuáles son las ventajas del aprendizaje supervisado?")
print(response)
\end{minted}

\subsection*{Conclusión}
\label{sec:orgc6298cc}
Ollama es una herramienta poderosa en el ámbito de la inteligencia artificial, diseñada para optimizar el procesamiento y la generación de texto en diversas aplicaciones. Su facilidad de integración y personalización la convierten en una opción ideal para desarrolladores y empresas que buscan soluciones basadas en NLP.





\section*{Spacy}
\label{sec:orgdd6c2ab}

\begin{minted}[]{python}
import spacy

# Cargar modelo de lenguaje en español
nlp = spacy.load("es_core_news_md")

# Texto de ejemplo (puede ser extraído de la Constitución)
texto = """
El Instituto Nacional Electoral (INE) es un organismo público autónomo encargado de organizar elecciones en México. 
Su autonomía está garantizada por el artículo 41 de la Constitución.
"""

# Procesar el texto
doc = nlp(texto)

# Extraer entidades relevantes
for ent in doc.ents:
    print(f"Entidad: {ent.text}, Tipo: {ent.label_}")

# Buscar términos clave
for token in doc:
    if token.text.lower() in ["autonomía", "independencia", "organismo"]:
        print(f"Término clave encontrado: {token.text}, Contexto: {token.sent}")

\end{minted}


\section*{Proyectos IA Final}
\label{sec:org469170f}

\subsection*{Actividad 1}
\label{sec:org5d848e5}
\subsubsection*{Implementación del Algoritmo A*}
\label{sec:org4fbe055}
Desarrollar el algoritmo A* utilizando el cascarón proporcionado en el apartado de \href{https://ealcaraz85.github.io/IA.io/\#org0d76d38}{pygames}. 

\begin{itemize}
\item Requisitos
\label{sec:org3d7609b}
\begin{itemize}
\item La solución debe ser óptima.
\item Indicar la lista cerrada al final del proceso.
\end{itemize}
\end{itemize}

\subsection*{Actividad 2}
\label{sec:org2394c44}
\subsubsection*{Solución basada en Árboles de Decisión y Redes Neuronales Multicapa}
\label{sec:org061c609}
A partir de la adaptación del juego de Phaser a Python:

\begin{itemize}
\item Tareas
\label{sec:org26a67b5}
\begin{enumerate}
\item Implementar la solución utilizando árboles de decisión.
\item Implementar la solución utilizando redes neuronales multicapa.
\end{enumerate}

\item Objetivo
\label{sec:org4c8eae5}
El jugador debe esquivar una pelota saltando.

\item Ejemplo de Redes Neuronales Multicapa en Python
\label{sec:org34a323e}
\begin{minted}[]{python}
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split

# Generar datos artificiales
np.random.seed(0)
X = np.random.rand(1000, 2)  # 1000 puntos con 2 características cada uno
y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Etiqueta 1 si la suma de las características > 1, de lo contrario 0

# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Crear el modelo de red neuronal multicapa
model = Sequential([
    Dense(4, input_dim=2, activation='relu'),  # Capa oculta con 4 neuronas y activación ReLU
    Dense(1, activation='sigmoid')            # Capa de salida con 1 neurona y activación sigmoide
])

# Compilar el modelo
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Entrenar el modelo
model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)

# Evaluar el modelo en el conjunto de prueba
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"\nPrecisión en el conjunto de prueba: {accuracy:.2f}")

# Probar con un nuevo dato
nuevo_dato = np.array([[0.8, 0.3]])  # Ejemplo con características específicas
prediccion = model.predict(nuevo_dato)
print(f"Predicción para {nuevo_dato}: {prediccion[0][0]:.2f}")
\end{minted}

\item Descripción del ejemplo
\label{sec:org9332bbb}
\begin{enumerate}
\item \textbf{\textbf{Datos artificiales}}:
\begin{itemize}
\item Se generan 1000 puntos en 2D, cada uno con dos características.
\item Las etiquetas se asignan como 1 si la suma de las características es mayor que 1, de lo contrario 0.
\end{itemize}
\item \textbf{\textbf{Modelo de red neuronal}}:
\begin{itemize}
\item Capa oculta: Tiene 4 neuronas con activación ReLU.
\item Capa de salida: Tiene 1 neurona con activación sigmoide, lo que da como resultado una probabilidad entre 0 y 1.
\end{itemize}
\item \textbf{\textbf{Compilación}}:
\begin{itemize}
\item Función de pérdida: `binary\textsubscript{crossentropy}`, adecuada para clasificación binaria.
\item Métrica: `accuracy` (precisión).
\end{itemize}
\item \textbf{\textbf{Predicción}}:
\begin{itemize}
\item Se pasa un nuevo punto al modelo y se obtiene una probabilidad. Si está cerca de 1, la salida es 1; si está cerca de 0, la salida es 0.
\end{itemize}
\end{enumerate}
\end{itemize}

\subsection*{Actividad 3}
\label{sec:org2cfabe3}
\subsubsection*{Identificación de modelos de autos con CNN}
\label{sec:orgb1612b6}
Utilizando el archivo `CNNriesgo` que se encuentra en la carpeta de Dropbox:

\begin{itemize}
\item Actividad
\label{sec:orgf2f545b}
Ajustar el dataset para identificar cinco modelos diferentes de autos.

\item Evaluación
\label{sec:orgcf24977}
\begin{itemize}
\item Herramientas utilizadas para la creación del dataset.
\item Precisión con la que se detectan los modelos.
\end{itemize}
\end{itemize}

\subsection*{Actividad 4}
\label{sec:orgeb61e74}
\subsubsection*{Fundamentación sobre la Reforma al Poder Judicial y Organismos Autónomos}
\label{sec:org38448db}
Utilizando algoritmos de inteligencia artificial, responder las siguientes preguntas y fundamentar si estás a favor o en contra de la reforma al poder judicial y a los organismos autónomos.

\begin{itemize}
\item Preguntas para la Ley del Poder Judicial
\label{sec:org13156cd}
\begin{enumerate}
\item ¿El diagnóstico de la ley al poder judicial es conocido y qué estudios expertos se tuvieron en cuenta?
\item ¿Por qué la reforma no incluyó a las fiscalías y a la defensoría, limitándose solo al poder judicial?
\item ¿Qué medidas concretas se implementarán para evitar la captación del crimen organizado y la violencia en el contexto electoral?
\item ¿Cómo garantizar que juristas probos y honestos se animen a competir públicamente frente a los riesgos de la violencia?
\item ¿Cómo se conforman los comités de postulación?
\item ¿Cómo asegurar la carrera judicial?
\item ¿Cómo compatibilizar la incorporación de medidas para preservar la identidad de los jueces (conocidos en el sistema interamericano como "jueces sin rostro") con los estándares internacionales?
\item ¿Cómo impactará el costo económico de esta reforma en la promoción y el acceso a la justicia?
\end{enumerate}

\item Preguntas para la Ley de Organismos Autónomos
\label{sec:org18442ff}
\begin{enumerate}
\item ¿Es constitucional esta ley, considerando que algunos organismos autónomos están establecidos en la Constitución?
\item ¿Cómo afectaría la eliminación de estos organismos a la transparencia y rendición de cuentas del gobierno?
\item ¿Qué funciones críticas podrían perder independencia y control al pasar al poder ejecutivo u otras instituciones?
\item ¿Existen alternativas para mejorar la eficiencia de los organismos autónomos sin eliminarlos?
\item ¿Qué sectores de la sociedad civil y grupos de interés se verían afectados por la desaparición de estos organismos?
\end{enumerate}

\item Puntos a evaluar
\label{sec:orgb9e4ca2}
\begin{itemize}
\item Algoritmos utilizados.
\item Herramientas generadas para el análisis.
\item Datos utilizados para la fundamentación.
\item Proceso de análisis.
\end{itemize}
\end{itemize}




\section*{Bibliografía}
\label{sec:org2247b9a}
\bibliography{bibliografia}



snscrape facebook-page "\url{https://www.facebook.com/share/v/19XLiKLLL8/}" --jsonl > posts.jsonl

snscrape facebook-page "\url{https://www.facebook.com/eduardo.alcc}" --jsonl > posts.jsonl

\url{https://www.facebook.com/share/v/19XLiKLLL8/}






\section*{Examen}
\label{sec:orgf90809c}

\begin{enumerate}
\item ¿De qué manera podría la inteligencia artificial ayudar a analizar las
ventajas y desventajas de eliminar organismos autónomos en México,
considerando su impacto en la transparencia y eficiencia
gubernamental?

\item ¿Qué indicadores podría identificar la IA para medir los posibles
efectos de la elección popular de jueces, magistrados y ministros en
la independencia judicial y en la confianza pública hacia el sistema
de justicia?

\item ¿Cómo podría la IA analizar casos internacionales donde se han
implementado reformas similares para identificar riesgos y beneficios
aplicables al contexto mexicano?

\item ¿Qué datos podrían extraerse mediante IA para evaluar si la
desaparición de organismos autónomos afecta el acceso a derechos
fundamentales como la protección de datos personales o la
transparencia?

\item ¿Cómo puede la IA ayudar a simular o predecir los efectos de estas
reformas en la percepción ciudadana sobre el poder judicial y los
organismos autónomos en términos de equidad y eficacia?
\end{enumerate}
\end{document}